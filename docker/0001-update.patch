From f8fad9fefc56c0c93a11966d840229c945d03e4d Mon Sep 17 00:00:00 2001
From: naomori <naoki.morita@gmail.com>
Date: Sat, 26 Oct 2019 01:21:38 +0900
Subject: [PATCH 1/2] update

---
 src/lib/datasets/dataset/arc.py               | 116 +++++
 src/lib/datasets/dataset_factory.py           |   4 +
 src/lib/datasets/sample/arc.py                | 149 +++++++
 src/lib/models/networks/DCNv2/.gitignore      |   5 +-
 src/lib/models/networks/DCNv2/LICENSE         |   2 +-
 src/lib/models/networks/DCNv2/README.md       |   7 +-
 src/lib/models/networks/DCNv2/build.py        |  43 --
 src/lib/models/networks/DCNv2/build_double.py |  43 --
 src/lib/models/networks/DCNv2/dcn_v2.py       | 252 ++++++++---
 src/lib/models/networks/DCNv2/dcn_v2_func.py  | 146 -------
 src/lib/models/networks/DCNv2/make.sh         |  14 +-
 src/lib/models/networks/DCNv2/setup.py        |  66 +++
 .../networks/DCNv2/src/cpu/dcn_v2_cpu.cpp     |  74 ++++
 .../models/networks/DCNv2/src/cpu/vision.h    |  60 +++
 .../networks/DCNv2/src/cuda/dcn_v2_cuda.cu    | 336 +++++++++++++++
 .../DCNv2/src/cuda/dcn_v2_im2col_cuda.cu      |  29 +-
 .../DCNv2/src/cuda/dcn_v2_im2col_cuda.h       |   1 +
 .../src/cuda/dcn_v2_im2col_cuda_double.cu     | 399 ------------------
 .../src/cuda/dcn_v2_im2col_cuda_double.h      | 100 -----
 .../src/cuda/dcn_v2_psroi_pooling_cuda.cu     | 394 ++++++++++-------
 .../src/cuda/dcn_v2_psroi_pooling_cuda.h      |  66 ---
 .../cuda/dcn_v2_psroi_pooling_cuda_double.cu  | 368 ----------------
 .../cuda/dcn_v2_psroi_pooling_cuda_double.h   |  66 ---
 .../models/networks/DCNv2/src/cuda/vision.h   |  60 +++
 src/lib/models/networks/DCNv2/src/dcn_v2.c    |  30 --
 src/lib/models/networks/DCNv2/src/dcn_v2.h    | 165 +++++++-
 .../models/networks/DCNv2/src/dcn_v2_cuda.c   | 335 ---------------
 .../models/networks/DCNv2/src/dcn_v2_cuda.h   |  60 ---
 .../networks/DCNv2/src/dcn_v2_cuda_double.c   | 358 ----------------
 .../networks/DCNv2/src/dcn_v2_cuda_double.h   |  61 ---
 .../models/networks/DCNv2/src/dcn_v2_double.c |  30 --
 .../models/networks/DCNv2/src/dcn_v2_double.h |  20 -
 src/lib/models/networks/DCNv2/src/vision.cpp  |   9 +
 src/lib/models/networks/DCNv2/test.py         | 149 ++++---
 34 files changed, 1547 insertions(+), 2470 deletions(-)
 create mode 100644 src/lib/datasets/dataset/arc.py
 create mode 100644 src/lib/datasets/sample/arc.py
 delete mode 100644 src/lib/models/networks/DCNv2/build.py
 delete mode 100644 src/lib/models/networks/DCNv2/build_double.py
 delete mode 100644 src/lib/models/networks/DCNv2/dcn_v2_func.py
 create mode 100644 src/lib/models/networks/DCNv2/setup.py
 create mode 100644 src/lib/models/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp
 create mode 100644 src/lib/models/networks/DCNv2/src/cpu/vision.h
 create mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_cuda.cu
 delete mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.cu
 delete mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.h
 delete mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.h
 delete mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.cu
 delete mode 100644 src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.h
 create mode 100644 src/lib/models/networks/DCNv2/src/cuda/vision.h
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2.c
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_cuda.c
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_cuda.h
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.c
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.h
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_double.c
 delete mode 100644 src/lib/models/networks/DCNv2/src/dcn_v2_double.h
 create mode 100644 src/lib/models/networks/DCNv2/src/vision.cpp

diff --git a/src/lib/datasets/dataset/arc.py b/src/lib/datasets/dataset/arc.py
new file mode 100644
index 0000000..5941a51
--- /dev/null
+++ b/src/lib/datasets/dataset/arc.py
@@ -0,0 +1,116 @@
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pycocotools.coco as coco
+from pycocotools.cocoeval import COCOeval
+import numpy as np
+import json
+import os
+
+import torch.utils.data as data
+
+class ARC(data.Dataset):
+    num_classes = 40
+    default_resolution = [1280, 960]
+    mean = np.array([0.36490161, 0.38790256, 0.42305998],
+                    dtype=np.float32).reshape(1, 1, 3)
+    std = np.array([0.20007855, 0.28563227, 0.31387719],
+                   dtype=np.float32).reshape(1, 1, 3)
+
+    def __init__(self, opt, split):
+        super(ARC, self).__init__()
+        self.data_dir = os.path.join(opt.data_dir, 'arc')
+        self.img_dir = os.path.join(self.data_dir, '{}'.format(split))
+        if split == 'test':
+            self.annot_path = os.path.join(
+                self.data_dir, 'annotations',
+                'val_arc.json').format(split)
+        else:
+            self.annot_path = os.path.join(
+                self.data_dir, 'annotations',
+                '{}_arc.json').format(split)
+        self.max_objs = 128
+        self.class_name = [
+            '__background__', 'Binder', 'Balloons', 'Baby_Wipes',
+            'Toilet_Brush', 'Toothbrushes', 'Crayons', 'Salts', 'DVD',
+            'Glue_Sticks', 'Eraser', 'Scissors', 'Green_Book', 'Socks',
+            'Irish_Spring', 'Paper_Tape', 'Touch_Tissues', 'Knit_Gloves',
+            'Laugh_Out_Loud_Jokes', 'Pencil_Cup', 'Mini_Marbles',
+            'Neoprene_Weight', 'Wine_Glasses', 'Water_Bottle', 'Reynolds_Pie',
+            'Reynolds_Wrap', 'Robots_Everywhere', 'Duct_Tape', 'Sponges',
+            'Speed_Stick', 'Index_Cards', 'Ice_Cube_Tray', 'Table_Cover',
+            'Measuring_Spoons', 'Bath_Sponge', 'Pencils', 'Mousetraps',
+            'Face_Cloth', 'Tennis_Balls', 'Spray_Bottle', 'Flashlights']
+        self._valid_ids = [
+            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13,
+            14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
+            24, 25, 27, 28, 31, 32, 33, 34, 35, 36,
+            37, 38, 39, 40]
+        self.cat_ids = {v: i for i, v in enumerate(self._valid_ids)}
+        self.voc_color = [(v // 32 * 64 + 64, (v // 8) % 4 * 64, v % 8 * 32) \
+                          for v in range(1, self.num_classes + 1)]
+        self._data_rng = np.random.RandomState(123)
+        self._eig_val = np.array([0.2141788, 0.01817699, 0.00341571],
+                                 dtype=np.float32)
+        self._eig_vec = np.array([
+            [-0.58752847, -0.69563484, 0.41340352],
+            [-0.5832747, 0.00994535, -0.81221408],
+            [-0.56089297, 0.71832671, 0.41158938]
+        ], dtype=np.float32)
+
+        self.split = split
+        self.opt = opt
+
+        print('==> initializing arc 2017 {} data.'.format(split))
+        self.coco = coco.COCO(self.annot_path)
+        self.images = self.coco.getImgIds()
+        self.num_samples = len(self.images)
+
+        print('Loaded {} {} samples'.format(split, self.num_samples))
+
+    def _to_float(self, x):
+        return float("{:.2f}".format(x))
+
+
+    def convert_eval_format(self, all_bboxes):
+        # import pdb; pdb.set_trace()
+        detections = []
+        for image_id in all_bboxes:
+            for cls_ind in all_bboxes[image_id]:
+                category_id = self._valid_ids[cls_ind - 1]
+                for bbox in all_bboxes[image_id][cls_ind]:
+                    bbox[2] -= bbox[0]
+                    bbox[3] -= bbox[1]
+                    score = bbox[4]
+                    bbox_out = list(map(self._to_float, bbox[0:4]))
+
+                    detection = {
+                        "image_id": int(image_id),
+                        "category_id": int(category_id),
+                        "bbox": bbox_out,
+                        "score": float("{:.2f}".format(score))
+                    }
+                    if len(bbox) > 5:
+                        extreme_points = list(map(self._to_float, bbox[5:13]))
+                        detection["extreme_points"] = extreme_points
+                    detections.append(detection)
+        return detections
+
+    def __len__(self):
+        return self.num_samples
+
+    def save_results(self, results, save_dir):
+        json.dump(self.convert_eval_format(results),
+                  open('{}/results.json'.format(save_dir), 'w'))
+
+    def run_eval(self, results, save_dir):
+        # result_json = os.path.join(save_dir, "results.json")
+        # detections  = self.convert_eval_format(results)
+        # json.dump(detections, open(result_json, "w"))
+        self.save_results(results, save_dir)
+        coco_dets = self.coco.loadRes('{}/results.json'.format(save_dir))
+        coco_eval = COCOeval(self.coco, coco_dets, "bbox")
+        coco_eval.evaluate()
+        coco_eval.accumulate()
+        coco_eval.summarize()
diff --git a/src/lib/datasets/dataset_factory.py b/src/lib/datasets/dataset_factory.py
index 007f853..05d72f7 100644
--- a/src/lib/datasets/dataset_factory.py
+++ b/src/lib/datasets/dataset_factory.py
@@ -5,9 +5,11 @@ from __future__ import print_function
 from .sample.ddd import DddDataset
 from .sample.exdet import EXDetDataset
 from .sample.ctdet import CTDetDataset
+from .sample.arc import ARCDataset
 from .sample.multi_pose import MultiPoseDataset
 
 from .dataset.coco import COCO
+from .dataset.arc import ARC
 from .dataset.pascal import PascalVOC
 from .dataset.kitti import KITTI
 from .dataset.coco_hp import COCOHP
@@ -15,6 +17,7 @@ from .dataset.coco_hp import COCOHP
 
 dataset_factory = {
   'coco': COCO,
+  'arc': ARC,
   'pascal': PascalVOC,
   'kitti': KITTI,
   'coco_hp': COCOHP
@@ -22,6 +25,7 @@ dataset_factory = {
 
 _sample_factory = {
   'exdet': EXDetDataset,
+  'arc': ARCDataset,
   'ctdet': CTDetDataset,
   'ddd': DddDataset,
   'multi_pose': MultiPoseDataset
diff --git a/src/lib/datasets/sample/arc.py b/src/lib/datasets/sample/arc.py
new file mode 100644
index 0000000..5e2640f
--- /dev/null
+++ b/src/lib/datasets/sample/arc.py
@@ -0,0 +1,149 @@
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import torch.utils.data as data
+import numpy as np
+import torch
+import json
+import cv2
+import os
+from utils.image import flip, color_aug
+from utils.image import get_affine_transform, affine_transform
+from utils.image import gaussian_radius, draw_umich_gaussian, draw_msra_gaussian
+from utils.image import draw_dense_reg
+import math
+
+class ARCDataset(data.Dataset):
+  def _coco_box_to_bbox(self, box):
+    bbox = np.array([box[0], box[1], box[0] + box[2], box[1] + box[3]],
+                    dtype=np.float32)
+    return bbox
+
+  def _get_border(self, border, size):
+    i = 1
+    while size - border // i <= border // i:
+        i *= 2
+    return border // i
+
+  def __getitem__(self, index):
+      img_id = self.images[index]
+      file_name = self.coco.loadImgs(ids=[img_id])[0]['file_name']
+      img_path = os.path.join(self.img_dir, file_name)
+      ann_ids = self.coco.getAnnIds(imgIds=[img_id])
+      anns = self.coco.loadAnns(ids=ann_ids)
+      num_objs = min(len(anns), self.max_objs)
+
+      img = cv2.imread(img_path)
+
+      height, width = img.shape[0], img.shape[1]
+      c = np.array([img.shape[1] / 2., img.shape[0] / 2.], dtype=np.float32)
+      if self.opt.keep_res:
+          input_h = (height | self.opt.pad) + 1
+          input_w = (width | self.opt.pad) + 1
+          s = np.array([input_w, input_h], dtype=np.float32)
+      else:
+          s = max(img.shape[0], img.shape[1]) * 1.0
+          input_h, input_w = self.opt.input_h, self.opt.input_w
+
+      flipped = False
+      if self.split == 'train':
+          if not self.opt.not_rand_crop:
+              s = s * np.random.choice(np.arange(0.6, 1.4, 0.1))
+              w_border = self._get_border(128, img.shape[1])
+              h_border = self._get_border(128, img.shape[0])
+              c[0] = np.random.randint(low=w_border,
+                                       high=img.shape[1] - w_border)
+              c[1] = np.random.randint(low=h_border,
+                                       high=img.shape[0] - h_border)
+          else:
+              sf = self.opt.scale
+              cf = self.opt.shift
+              c[0] += s * np.clip(np.random.randn() * cf, -2 * cf, 2 * cf)
+              c[1] += s * np.clip(np.random.randn() * cf, -2 * cf, 2 * cf)
+              s = s * np.clip(np.random.randn() * sf + 1, 1 - sf, 1 + sf)
+
+          if np.random.random() < self.opt.flip:
+              flipped = True
+              img = img[:, ::-1, :]
+              c[0] = width - c[0] - 1
+
+      trans_input = get_affine_transform(
+          c, s, 0, [input_w, input_h])
+      inp = cv2.warpAffine(img, trans_input,
+                           (input_w, input_h),
+                           flags=cv2.INTER_LINEAR)
+      inp = (inp.astype(np.float32) / 255.)
+      if self.split == 'train' and not self.opt.no_color_aug:
+          color_aug(self._data_rng, inp, self._eig_val, self._eig_vec)
+      inp = (inp - self.mean) / self.std
+      inp = inp.transpose(2, 0, 1)
+
+      output_h = input_h // self.opt.down_ratio
+      output_w = input_w // self.opt.down_ratio
+      num_classes = self.num_classes
+      trans_output = get_affine_transform(c, s, 0, [output_w, output_h])
+
+      hm = np.zeros((num_classes, output_h, output_w), dtype=np.float32)
+      wh = np.zeros((self.max_objs, 2), dtype=np.float32)
+      dense_wh = np.zeros((2, output_h, output_w), dtype=np.float32)
+      reg = np.zeros((self.max_objs, 2), dtype=np.float32)
+      ind = np.zeros((self.max_objs), dtype=np.int64)
+      reg_mask = np.zeros((self.max_objs), dtype=np.uint8)
+      cat_spec_wh = np.zeros((self.max_objs, num_classes * 2), dtype=np.float32)
+      cat_spec_mask = np.zeros((self.max_objs, num_classes * 2), dtype=np.uint8)
+
+      draw_gaussian = draw_msra_gaussian if self.opt.mse_loss else \
+          draw_umich_gaussian
+
+      gt_det = []
+      for k in range(num_objs):
+          ann = anns[k]
+          bbox = self._coco_box_to_bbox(ann['bbox'])
+          cls_id = int(self.cat_ids[ann['category_id']])
+          if flipped:
+              bbox[[0, 2]] = width - bbox[[2, 0]] - 1
+          bbox[:2] = affine_transform(bbox[:2], trans_output)
+          bbox[2:] = affine_transform(bbox[2:], trans_output)
+          bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, output_w - 1)
+          bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, output_h - 1)
+          h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]
+          if h > 0 and w > 0:
+              radius = gaussian_radius((math.ceil(h), math.ceil(w)))
+              radius = max(0, int(radius))
+              radius = self.opt.hm_gauss if self.opt.mse_loss else radius
+              ct = np.array(
+                  [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2],
+                  dtype=np.float32)
+              ct_int = ct.astype(np.int32)
+              draw_gaussian(hm[cls_id], ct_int, radius)
+              wh[k] = 1. * w, 1. * h
+              ind[k] = ct_int[1] * output_w + ct_int[0]
+              reg[k] = ct - ct_int
+              reg_mask[k] = 1
+              cat_spec_wh[k, cls_id * 2: cls_id * 2 + 2] = wh[k]
+              cat_spec_mask[k, cls_id * 2: cls_id * 2 + 2] = 1
+              if self.opt.dense_wh:
+                  draw_dense_reg(dense_wh, hm.max(axis=0), ct_int, wh[k],
+                                 radius)
+              gt_det.append([ct[0] - w / 2, ct[1] - h / 2,
+                             ct[0] + w / 2, ct[1] + h / 2, 1, cls_id])
+
+      ret = {'input': inp, 'hm': hm, 'reg_mask': reg_mask, 'ind': ind, 'wh': wh}
+      if self.opt.dense_wh:
+          hm_a = hm.max(axis=0, keepdims=True)
+          dense_wh_mask = np.concatenate([hm_a, hm_a], axis=0)
+          ret.update({'dense_wh': dense_wh, 'dense_wh_mask': dense_wh_mask})
+          del ret['wh']
+      elif self.opt.cat_spec_wh:
+          ret.update(
+              {'cat_spec_wh': cat_spec_wh, 'cat_spec_mask': cat_spec_mask})
+          del ret['wh']
+      if self.opt.reg_offset:
+          ret.update({'reg': reg})
+      if self.opt.debug > 0 or not self.split == 'train':
+          gt_det = np.array(gt_det, dtype=np.float32) if len(gt_det) > 0 else \
+              np.zeros((1, 6), dtype=np.float32)
+          meta = {'c': c, 's': s, 'gt_det': gt_det, 'img_id': img_id}
+          ret['meta'] = meta
+      return ret
diff --git a/src/lib/models/networks/DCNv2/.gitignore b/src/lib/models/networks/DCNv2/.gitignore
index b1e9421..73296c0 100644
--- a/src/lib/models/networks/DCNv2/.gitignore
+++ b/src/lib/models/networks/DCNv2/.gitignore
@@ -3,4 +3,7 @@
 *.so
 *.o
 *pyc
-_ext
\ No newline at end of file
+_ext
+build
+DCNv2.egg-info
+dist
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/LICENSE b/src/lib/models/networks/DCNv2/LICENSE
index cd31b28..b2e3b52 100644
--- a/src/lib/models/networks/DCNv2/LICENSE
+++ b/src/lib/models/networks/DCNv2/LICENSE
@@ -26,4 +26,4 @@ DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/README.md b/src/lib/models/networks/DCNv2/README.md
index 0ddcf18..9787cfa 100644
--- a/src/lib/models/networks/DCNv2/README.md
+++ b/src/lib/models/networks/DCNv2/README.md
@@ -1,4 +1,4 @@
-## Deformable Convolutional Networks V2 with Pytorch
+## Deformable Convolutional Networks V2 with Pytorch 1.0
 
 ### Build
 ```bash
@@ -38,6 +38,11 @@
 
     dout = dpooling(input, rois)
 ```
+### Note
+Now the master branch is for pytorch 1.0 (new ATen API), you can switch back to pytorch 0.4 with,
+```bash
+git checkout pytorch_0.4
+```
 
 ### Known Issues:
 
diff --git a/src/lib/models/networks/DCNv2/build.py b/src/lib/models/networks/DCNv2/build.py
deleted file mode 100644
index b93f2a9..0000000
--- a/src/lib/models/networks/DCNv2/build.py
+++ /dev/null
@@ -1,43 +0,0 @@
-import os
-import torch
-from torch.utils.ffi import create_extension
-
-
-sources = ['src/dcn_v2.c']
-headers = ['src/dcn_v2.h']
-defines = []
-with_cuda = False
-
-extra_objects = []
-if torch.cuda.is_available():
-    print('Including CUDA code.')
-    sources += ['src/dcn_v2_cuda.c']
-    headers += ['src/dcn_v2_cuda.h']
-    defines += [('WITH_CUDA', None)]
-    extra_objects += ['src/cuda/dcn_v2_im2col_cuda.cu.o']
-    extra_objects += ['src/cuda/dcn_v2_psroi_pooling_cuda.cu.o']
-    with_cuda = True
-else:
-    raise ValueError('CUDA is not available')
-
-extra_compile_args = ['-fopenmp', '-std=c99']
-
-this_file = os.path.dirname(os.path.realpath(__file__))
-print(this_file)
-sources = [os.path.join(this_file, fname) for fname in sources]
-headers = [os.path.join(this_file, fname) for fname in headers]
-extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]
-
-ffi = create_extension(
-    '_ext.dcn_v2',
-    headers=headers,
-    sources=sources,
-    define_macros=defines,
-    relative_to=__file__,
-    with_cuda=with_cuda,
-    extra_objects=extra_objects,
-    extra_compile_args=extra_compile_args
-)
-
-if __name__ == '__main__':
-    ffi.build()
diff --git a/src/lib/models/networks/DCNv2/build_double.py b/src/lib/models/networks/DCNv2/build_double.py
deleted file mode 100644
index 02f3912..0000000
--- a/src/lib/models/networks/DCNv2/build_double.py
+++ /dev/null
@@ -1,43 +0,0 @@
-import os
-import torch
-from torch.utils.ffi import create_extension
-
-
-sources = ['src/dcn_v2_double.c']
-headers = ['src/dcn_v2_double.h']
-defines = []
-with_cuda = False
-
-extra_objects = []
-if torch.cuda.is_available():
-    print('Including CUDA code.')
-    sources += ['src/dcn_v2_cuda_double.c']
-    headers += ['src/dcn_v2_cuda_double.h']
-    defines += [('WITH_CUDA', None)]
-    extra_objects += ['src/cuda/dcn_v2_im2col_cuda_double.cu.o']
-    extra_objects += ['src/cuda/dcn_v2_psroi_pooling_cuda_double.cu.o']
-    with_cuda = True
-else:
-    raise ValueError('CUDA is not available')
-
-extra_compile_args = ['-fopenmp', '-std=c99']
-
-this_file = os.path.dirname(os.path.realpath(__file__))
-print(this_file)
-sources = [os.path.join(this_file, fname) for fname in sources]
-headers = [os.path.join(this_file, fname) for fname in headers]
-extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]
-
-ffi = create_extension(
-    '_ext.dcn_v2_double',
-    headers=headers,
-    sources=sources,
-    define_macros=defines,
-    relative_to=__file__,
-    with_cuda=with_cuda,
-    extra_objects=extra_objects,
-    extra_compile_args=extra_compile_args
-)
-
-if __name__ == '__main__':
-    ffi.build()
diff --git a/src/lib/models/networks/DCNv2/dcn_v2.py b/src/lib/models/networks/DCNv2/dcn_v2.py
index e1bb700..982bef5 100644
--- a/src/lib/models/networks/DCNv2/dcn_v2.py
+++ b/src/lib/models/networks/DCNv2/dcn_v2.py
@@ -3,13 +3,56 @@ from __future__ import absolute_import
 from __future__ import print_function
 from __future__ import division
 
-import torch
 import math
+import torch
 from torch import nn
+from torch.autograd import Function
 from torch.nn.modules.utils import _pair
+from torch.autograd.function import once_differentiable
+
+import _ext as _backend
+
+
+class _DCNv2(Function):
+    @staticmethod
+    def forward(ctx, input, offset, mask, weight, bias,
+                stride, padding, dilation, deformable_groups):
+        ctx.stride = _pair(stride)
+        ctx.padding = _pair(padding)
+        ctx.dilation = _pair(dilation)
+        ctx.kernel_size = _pair(weight.shape[2:4])
+        ctx.deformable_groups = deformable_groups
+        output = _backend.dcn_v2_forward(input, weight, bias,
+                                         offset, mask,
+                                         ctx.kernel_size[0], ctx.kernel_size[1],
+                                         ctx.stride[0], ctx.stride[1],
+                                         ctx.padding[0], ctx.padding[1],
+                                         ctx.dilation[0], ctx.dilation[1],
+                                         ctx.deformable_groups)
+        ctx.save_for_backward(input, offset, mask, weight, bias)
+        return output
+
+    @staticmethod
+    @once_differentiable
+    def backward(ctx, grad_output):
+        input, offset, mask, weight, bias = ctx.saved_tensors
+        grad_input, grad_offset, grad_mask, grad_weight, grad_bias = \
+            _backend.dcn_v2_backward(input, weight,
+                                     bias,
+                                     offset, mask,
+                                     grad_output,
+                                     ctx.kernel_size[0], ctx.kernel_size[1],
+                                     ctx.stride[0], ctx.stride[1],
+                                     ctx.padding[0], ctx.padding[1],
+                                     ctx.dilation[0], ctx.dilation[1],
+                                     ctx.deformable_groups)
+
+        return grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\
+            None, None, None, None,
+
+
+dcn_v2_conv = _DCNv2.apply
 
-from .dcn_v2_func import DCNv2Function
-from .dcn_v2_func import DCNv2PoolingFunction
 
 class DCNv2(nn.Module):
 
@@ -19,12 +62,13 @@ class DCNv2(nn.Module):
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.kernel_size = _pair(kernel_size)
-        self.stride = stride
-        self.padding = padding
-        self.dilation = dilation
+        self.stride = _pair(stride)
+        self.padding = _pair(padding)
+        self.dilation = _pair(dilation)
         self.deformable_groups = deformable_groups
 
-        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))
+        self.weight = nn.Parameter(torch.Tensor(
+            out_channels, in_channels, *self.kernel_size))
         self.bias = nn.Parameter(torch.Tensor(out_channels))
         self.reset_parameters()
 
@@ -37,8 +81,17 @@ class DCNv2(nn.Module):
         self.bias.data.zero_()
 
     def forward(self, input, offset, mask):
-        func = DCNv2Function(self.stride, self.padding, self.dilation, self.deformable_groups)
-        return func(input, offset, mask, self.weight, self.bias)
+        assert 2 * self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \
+            offset.shape[1]
+        assert self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \
+            mask.shape[1]
+        return dcn_v2_conv(input, offset, mask,
+                           self.weight,
+                           self.bias,
+                           self.stride,
+                           self.padding,
+                           self.dilation,
+                           self.deformable_groups)
 
 
 class DCN(DCNv2):
@@ -49,11 +102,12 @@ class DCN(DCNv2):
         super(DCN, self).__init__(in_channels, out_channels,
                                   kernel_size, stride, padding, dilation, deformable_groups)
 
+        channels_ = self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1]
         self.conv_offset_mask = nn.Conv2d(self.in_channels,
-                                          self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1],
+                                          channels_,
                                           kernel_size=self.kernel_size,
-                                          stride=(self.stride, self.stride),
-                                          padding=(self.padding, self.padding),
+                                          stride=self.stride,
+                                          padding=self.padding,
                                           bias=True)
         self.init_offset()
 
@@ -66,8 +120,68 @@ class DCN(DCNv2):
         o1, o2, mask = torch.chunk(out, 3, dim=1)
         offset = torch.cat((o1, o2), dim=1)
         mask = torch.sigmoid(mask)
-        func = DCNv2Function(self.stride, self.padding, self.dilation, self.deformable_groups)
-        return func(input, offset, mask, self.weight, self.bias)
+        return dcn_v2_conv(input, offset, mask,
+                           self.weight, self.bias,
+                           self.stride,
+                           self.padding,
+                           self.dilation,
+                           self.deformable_groups)
+
+
+
+class _DCNv2Pooling(Function):
+    @staticmethod
+    def forward(ctx, input, rois, offset,
+                spatial_scale,
+                pooled_size,
+                output_dim,
+                no_trans,
+                group_size=1,
+                part_size=None,
+                sample_per_part=4,
+                trans_std=.0):
+        ctx.spatial_scale = spatial_scale
+        ctx.no_trans = int(no_trans)
+        ctx.output_dim = output_dim
+        ctx.group_size = group_size
+        ctx.pooled_size = pooled_size
+        ctx.part_size = pooled_size if part_size is None else part_size
+        ctx.sample_per_part = sample_per_part
+        ctx.trans_std = trans_std
+
+        output, output_count = \
+            _backend.dcn_v2_psroi_pooling_forward(input, rois, offset,
+                                                  ctx.no_trans, ctx.spatial_scale,
+                                                  ctx.output_dim, ctx.group_size,
+                                                  ctx.pooled_size, ctx.part_size,
+                                                  ctx.sample_per_part, ctx.trans_std)
+        ctx.save_for_backward(input, rois, offset, output_count)
+        return output
+
+    @staticmethod
+    @once_differentiable
+    def backward(ctx, grad_output):
+        input, rois, offset, output_count = ctx.saved_tensors
+        grad_input, grad_offset = \
+            _backend.dcn_v2_psroi_pooling_backward(grad_output,
+                                                   input,
+                                                   rois,
+                                                   offset,
+                                                   output_count,
+                                                   ctx.no_trans,
+                                                   ctx.spatial_scale,
+                                                   ctx.output_dim,
+                                                   ctx.group_size,
+                                                   ctx.pooled_size,
+                                                   ctx.part_size,
+                                                   ctx.sample_per_part,
+                                                   ctx.trans_std)
+
+        return grad_input, None, grad_offset, \
+            None, None, None, None, None, None, None, None
+
+
+dcn_v2_pooling = _DCNv2Pooling.apply
 
 
 class DCNv2Pooling(nn.Module):
@@ -90,20 +204,21 @@ class DCNv2Pooling(nn.Module):
         self.part_size = pooled_size if part_size is None else part_size
         self.sample_per_part = sample_per_part
         self.trans_std = trans_std
-        self.func = DCNv2PoolingFunction(self.spatial_scale,
-                             self.pooled_size,
-                             self.output_dim,
-                             self.no_trans,
-                             self.group_size,
-                             self.part_size,
-                             self.sample_per_part,
-                             self.trans_std)
-
-    def forward(self, data, rois, offset):
 
+    def forward(self, input, rois, offset):
+        assert input.shape[1] == self.output_dim
         if self.no_trans:
-            offset = data.new()
-        return self.func(data, rois, offset)
+            offset = input.new()
+        return dcn_v2_pooling(input, rois, offset,
+                              self.spatial_scale,
+                              self.pooled_size,
+                              self.output_dim,
+                              self.no_trans,
+                              self.group_size,
+                              self.part_size,
+                              self.sample_per_part,
+                              self.trans_std)
+
 
 class DCNPooling(DCNv2Pooling):
 
@@ -129,43 +244,60 @@ class DCNPooling(DCNv2Pooling):
         self.deform_fc_dim = deform_fc_dim
 
         if not no_trans:
-            self.func_offset = DCNv2PoolingFunction(self.spatial_scale,
-                                                    self.pooled_size,
-                                                    self.output_dim,
-                                                    True,
-                                                    self.group_size,
-                                                    self.part_size,
-                                                    self.sample_per_part,
-                                                    self.trans_std)
-            self.offset_fc = nn.Sequential(
-                nn.Linear(self.pooled_size * self.pooled_size * self.output_dim, self.deform_fc_dim),
+            self.offset_mask_fc = nn.Sequential(
+                nn.Linear(self.pooled_size * self.pooled_size *
+                          self.output_dim, self.deform_fc_dim),
                 nn.ReLU(inplace=True),
                 nn.Linear(self.deform_fc_dim, self.deform_fc_dim),
                 nn.ReLU(inplace=True),
-                nn.Linear(self.deform_fc_dim, self.pooled_size * self.pooled_size * 2)
+                nn.Linear(self.deform_fc_dim, self.pooled_size *
+                          self.pooled_size * 3)
             )
-            self.offset_fc[4].weight.data.zero_()
-            self.offset_fc[4].bias.data.zero_()
-            self.mask_fc = nn.Sequential(
-                nn.Linear(self.pooled_size * self.pooled_size * self.output_dim, self.deform_fc_dim),
-                nn.ReLU(inplace=True),
-                nn.Linear(self.deform_fc_dim, self.pooled_size * self.pooled_size * 1),
-                nn.Sigmoid()
-            )
-            self.mask_fc[2].weight.data.zero_()
-            self.mask_fc[2].bias.data.zero_()
+            self.offset_mask_fc[4].weight.data.zero_()
+            self.offset_mask_fc[4].bias.data.zero_()
 
-    def forward(self, data, rois):
-        if self.no_trans:
-            offset = data.new()
-        else:
+    def forward(self, input, rois):
+        offset = input.new()
+
+        if not self.no_trans:
+
+            # do roi_align first
             n = rois.shape[0]
-            offset = data.new()
-            x = self.func_offset(data, rois, offset)
-            offset = self.offset_fc(x.view(n, -1))
-            offset = offset.view(n, 2, self.pooled_size, self.pooled_size)
-            mask = self.mask_fc(x.view(n, -1))
-            mask = mask.view(n, 1, self.pooled_size, self.pooled_size)
-            feat = self.func(data, rois, offset) * mask
-            return feat
-        return self.func(data, rois, offset)
+            roi = dcn_v2_pooling(input, rois, offset,
+                                 self.spatial_scale,
+                                 self.pooled_size,
+                                 self.output_dim,
+                                 True,  # no trans
+                                 self.group_size,
+                                 self.part_size,
+                                 self.sample_per_part,
+                                 self.trans_std)
+
+            # build mask and offset
+            offset_mask = self.offset_mask_fc(roi.view(n, -1))
+            offset_mask = offset_mask.view(
+                n, 3, self.pooled_size, self.pooled_size)
+            o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)
+            offset = torch.cat((o1, o2), dim=1)
+            mask = torch.sigmoid(mask)
+
+            # do pooling with offset and mask
+            return dcn_v2_pooling(input, rois, offset,
+                                  self.spatial_scale,
+                                  self.pooled_size,
+                                  self.output_dim,
+                                  self.no_trans,
+                                  self.group_size,
+                                  self.part_size,
+                                  self.sample_per_part,
+                                  self.trans_std) * mask
+        # only roi_align
+        return dcn_v2_pooling(input, rois, offset,
+                              self.spatial_scale,
+                              self.pooled_size,
+                              self.output_dim,
+                              self.no_trans,
+                              self.group_size,
+                              self.part_size,
+                              self.sample_per_part,
+                              self.trans_std)
diff --git a/src/lib/models/networks/DCNv2/dcn_v2_func.py b/src/lib/models/networks/DCNv2/dcn_v2_func.py
deleted file mode 100644
index 7e98f49..0000000
--- a/src/lib/models/networks/DCNv2/dcn_v2_func.py
+++ /dev/null
@@ -1,146 +0,0 @@
-#!/usr/bin/env python
-from __future__ import absolute_import
-from __future__ import print_function
-from __future__ import division
-
-import torch
-from torch.autograd import Function
-
-from ._ext import dcn_v2 as _backend
-# from _ext import dcn_v2_double as _backend
-
-
-class DCNv2Function(Function):
-
-    def __init__(self, stride, padding, dilation=1, deformable_groups=1):
-        super(DCNv2Function, self).__init__()
-        self.stride = stride
-        self.padding = padding
-        self.dilation = dilation
-        self.deformable_groups = deformable_groups
-
-    def forward(self, input, offset, mask, weight, bias):
-        if not input.is_cuda:
-            raise NotImplementedError
-        if weight.requires_grad or mask.requires_grad or offset.requires_grad or input.requires_grad:
-            self.save_for_backward(input, offset, mask, weight, bias)
-        output = input.new(*self._infer_shape(input, weight))
-        self._bufs = [input.new(), input.new()]
-        _backend.dcn_v2_cuda_forward(input, weight,
-                                     bias, self._bufs[0],
-                                     offset, mask,
-                                     output, self._bufs[1],
-                                     weight.shape[2], weight.shape[3],
-                                     self.stride, self.stride,
-                                     self.padding, self.padding,
-                                     self.dilation, self.dilation,
-                                     self.deformable_groups)
-        return output
-
-    def backward(self, grad_output):
-        if not grad_output.is_cuda:
-            raise NotImplementedError
-        input, offset, mask, weight, bias = self.saved_tensors
-        grad_input = input.new(*input.size()).zero_()
-        grad_offset = offset.new(*offset.size()).zero_()
-        grad_mask = mask.new(*mask.size()).zero_()
-        grad_weight = weight.new(*weight.size()).zero_()
-        grad_bias = bias.new(*bias.size()).zero_()
-        _backend.dcn_v2_cuda_backward(input, weight,
-                                      bias, self._bufs[0],
-                                      offset, mask,
-                                      self._bufs[1],
-                                      grad_input, grad_weight,
-                                      grad_bias, grad_offset,
-                                      grad_mask, grad_output,
-                                      weight.shape[2], weight.shape[3],
-                                      self.stride, self.stride,
-                                      self.padding, self.padding,
-                                      self.dilation, self.dilation,
-                                      self.deformable_groups)
-
-        return grad_input, grad_offset, grad_mask, grad_weight, grad_bias
-
-    def _infer_shape(self, input, weight):
-        n = input.size(0)
-        channels_out = weight.size(0)
-        height, width = input.shape[2:4]
-        kernel_h, kernel_w = weight.shape[2:4]
-        height_out = (height + 2 * self.padding -
-                      (self.dilation * (kernel_h - 1) + 1)) // self.stride + 1
-        width_out = (width + 2 * self.padding - (self.dilation *
-                                                 (kernel_w - 1) + 1)) // self.stride + 1
-        return (n, channels_out, height_out, width_out)
-
-
-class DCNv2PoolingFunction(Function):
-
-    def __init__(self,
-                 spatial_scale,
-                 pooled_size,
-                 output_dim,
-                 no_trans,
-                 group_size=1,
-                 part_size=None,
-                 sample_per_part=4,
-                 trans_std=.0):
-        super(DCNv2PoolingFunction, self).__init__()
-        self.spatial_scale = spatial_scale
-        self.pooled_size = pooled_size
-        self.output_dim = output_dim
-        self.no_trans = no_trans
-        self.group_size = group_size
-        self.part_size = pooled_size if part_size is None else part_size
-        self.sample_per_part = sample_per_part
-        self.trans_std = trans_std
-
-        assert self.trans_std >= 0.0 and self.trans_std <= 1.0
-
-    def forward(self, data, rois, offset):
-        if not data.is_cuda:
-            raise NotImplementedError
-
-        output = data.new(*self._infer_shape(data, rois))
-        output_count = data.new(*self._infer_shape(data, rois))
-        _backend.dcn_v2_psroi_pooling_cuda_forward(data, rois, offset,
-                                                   output, output_count,
-                                                   self.no_trans, self.spatial_scale,
-                                                   self.output_dim, self.group_size,
-                                                   self.pooled_size, self.part_size,
-                                                   self.sample_per_part, self.trans_std)
-
-        if data.requires_grad or rois.requires_grad or offset.requires_grad:
-            self.save_for_backward(data, rois, offset, output_count)
-
-        return output
-
-    def backward(self, grad_output):
-        if not grad_output.is_cuda:
-            raise NotImplementedError
-
-        data, rois, offset, output_count = self.saved_tensors
-        grad_input = data.new(*data.size()).zero_()
-        grad_offset = offset.new(*offset.size()).zero_()
-
-        _backend.dcn_v2_psroi_pooling_cuda_backward(grad_output,
-                                                    data,
-                                                    rois,
-                                                    offset,
-                                                    output_count,
-                                                    grad_input,
-                                                    grad_offset,
-                                                    self.no_trans,
-                                                    self.spatial_scale,
-                                                    self.output_dim,
-                                                    self.group_size,
-                                                    self.pooled_size,
-                                                    self.part_size,
-                                                    self.sample_per_part,
-                                                    self.trans_std)
-        return grad_input, None, grad_offset
-
-    def _infer_shape(self, data, rois):
-        # _, c, h, w = data.shape[:4]
-        c = data.shape[1]
-        n = rois.shape[0]
-        return (n, self.output_dim, self.pooled_size, self.pooled_size)
diff --git a/src/lib/models/networks/DCNv2/make.sh b/src/lib/models/networks/DCNv2/make.sh
index d489f7c..f1f15c0 100755
--- a/src/lib/models/networks/DCNv2/make.sh
+++ b/src/lib/models/networks/DCNv2/make.sh
@@ -1,14 +1,2 @@
 #!/usr/bin/env bash
-cd src/cuda
-
-# compile dcn
-nvcc -c -o dcn_v2_im2col_cuda.cu.o dcn_v2_im2col_cuda.cu -x cu -Xcompiler -fPIC
-nvcc -c -o dcn_v2_im2col_cuda_double.cu.o dcn_v2_im2col_cuda_double.cu -x cu -Xcompiler -fPIC
-
-# compile dcn-roi-pooling
-nvcc -c -o dcn_v2_psroi_pooling_cuda.cu.o dcn_v2_psroi_pooling_cuda.cu -x cu -Xcompiler -fPIC
-nvcc -c -o dcn_v2_psroi_pooling_cuda_double.cu.o dcn_v2_psroi_pooling_cuda_double.cu -x cu -Xcompiler -fPIC
-
-cd -
-python build.py
-python build_double.py
+python setup.py build develop
diff --git a/src/lib/models/networks/DCNv2/setup.py b/src/lib/models/networks/DCNv2/setup.py
new file mode 100644
index 0000000..1082494
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/setup.py
@@ -0,0 +1,66 @@
+#!/usr/bin/env python
+
+import os
+import glob
+
+import torch
+
+from torch.utils.cpp_extension import CUDA_HOME
+from torch.utils.cpp_extension import CppExtension
+from torch.utils.cpp_extension import CUDAExtension
+
+from setuptools import find_packages
+from setuptools import setup
+
+requirements = ["torch", "torchvision"]
+
+def get_extensions():
+    this_dir = os.path.dirname(os.path.abspath(__file__))
+    extensions_dir = os.path.join(this_dir, "src")
+
+    main_file = glob.glob(os.path.join(extensions_dir, "*.cpp"))
+    source_cpu = glob.glob(os.path.join(extensions_dir, "cpu", "*.cpp"))
+    source_cuda = glob.glob(os.path.join(extensions_dir, "cuda", "*.cu"))
+
+    sources = main_file + source_cpu
+    extension = CppExtension
+    extra_compile_args = {"cxx": []}
+    define_macros = []
+
+    if torch.cuda.is_available() and CUDA_HOME is not None:
+        extension = CUDAExtension
+        sources += source_cuda
+        define_macros += [("WITH_CUDA", None)]
+        extra_compile_args["nvcc"] = [
+            "-DCUDA_HAS_FP16=1",
+            "-D__CUDA_NO_HALF_OPERATORS__",
+            "-D__CUDA_NO_HALF_CONVERSIONS__",
+            "-D__CUDA_NO_HALF2_OPERATORS__",
+        ]
+    else:
+        raise NotImplementedError('Cuda is not availabel')
+
+    sources = [os.path.join(extensions_dir, s) for s in sources]
+    include_dirs = [extensions_dir]
+    ext_modules = [
+        extension(
+            "_ext",
+            sources,
+            include_dirs=include_dirs,
+            define_macros=define_macros,
+            extra_compile_args=extra_compile_args,
+        )
+    ]
+    return ext_modules
+
+setup(
+    name="DCNv2",
+    version="0.1",
+    author="charlesshang",
+    url="https://github.com/charlesshang/DCNv2",
+    description="deformable convolutional networks",
+    packages=find_packages(exclude=("configs", "tests",)),
+    # install_requires=requirements,
+    ext_modules=get_extensions(),
+    cmdclass={"build_ext": torch.utils.cpp_extension.BuildExtension},
+)
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp b/src/lib/models/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp
new file mode 100644
index 0000000..a68ccef
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp
@@ -0,0 +1,74 @@
+#include <vector>
+
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+
+
+at::Tensor
+dcn_v2_cpu_forward(const at::Tensor &input,
+                   const at::Tensor &weight,
+                   const at::Tensor &bias,
+                   const at::Tensor &offset,
+                   const at::Tensor &mask,
+                   const int kernel_h,
+                   const int kernel_w,
+                   const int stride_h,
+                   const int stride_w,
+                   const int pad_h,
+                   const int pad_w,
+                   const int dilation_h,
+                   const int dilation_w,
+                   const int deformable_group)
+{
+    AT_ERROR("Not implement on cpu");
+}
+
+std::vector<at::Tensor>
+dcn_v2_cpu_backward(const at::Tensor &input,
+                    const at::Tensor &weight,
+                    const at::Tensor &bias,
+                    const at::Tensor &offset,
+                    const at::Tensor &mask,
+                    const at::Tensor &grad_output,
+                    int kernel_h, int kernel_w,
+                    int stride_h, int stride_w,
+                    int pad_h, int pad_w,
+                    int dilation_h, int dilation_w,
+                    int deformable_group)
+{
+    AT_ERROR("Not implement on cpu");
+}
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cpu_forward(const at::Tensor &input,
+                                 const at::Tensor &bbox,
+                                 const at::Tensor &trans,
+                                 const int no_trans,
+                                 const float spatial_scale,
+                                 const int output_dim,
+                                 const int group_size,
+                                 const int pooled_size,
+                                 const int part_size,
+                                 const int sample_per_part,
+                                 const float trans_std)
+{
+    AT_ERROR("Not implement on cpu");
+}
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cpu_backward(const at::Tensor &out_grad,
+                                  const at::Tensor &input,
+                                  const at::Tensor &bbox,
+                                  const at::Tensor &trans,
+                                  const at::Tensor &top_count,
+                                  const int no_trans,
+                                  const float spatial_scale,
+                                  const int output_dim,
+                                  const int group_size,
+                                  const int pooled_size,
+                                  const int part_size,
+                                  const int sample_per_part,
+                                  const float trans_std)
+{
+    AT_ERROR("Not implement on cpu");
+}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cpu/vision.h b/src/lib/models/networks/DCNv2/src/cpu/vision.h
new file mode 100644
index 0000000..d5fbf1f
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/src/cpu/vision.h
@@ -0,0 +1,60 @@
+#pragma once
+#include <torch/extension.h>
+
+at::Tensor
+dcn_v2_cpu_forward(const at::Tensor &input,
+                    const at::Tensor &weight,
+                    const at::Tensor &bias,
+                    const at::Tensor &offset,
+                    const at::Tensor &mask,
+                    const int kernel_h,
+                    const int kernel_w,
+                    const int stride_h,
+                    const int stride_w,
+                    const int pad_h,
+                    const int pad_w,
+                    const int dilation_h,
+                    const int dilation_w,
+                    const int deformable_group);
+
+std::vector<at::Tensor>
+dcn_v2_cpu_backward(const at::Tensor &input,
+                     const at::Tensor &weight,
+                     const at::Tensor &bias,
+                     const at::Tensor &offset,
+                     const at::Tensor &mask,
+                     const at::Tensor &grad_output,
+                     int kernel_h, int kernel_w,
+                     int stride_h, int stride_w,
+                     int pad_h, int pad_w,
+                     int dilation_h, int dilation_w,
+                     int deformable_group);
+
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cpu_forward(const at::Tensor &input,
+                                  const at::Tensor &bbox,
+                                  const at::Tensor &trans,
+                                  const int no_trans,
+                                  const float spatial_scale,
+                                  const int output_dim,
+                                  const int group_size,
+                                  const int pooled_size,
+                                  const int part_size,
+                                  const int sample_per_part,
+                                  const float trans_std);
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cpu_backward(const at::Tensor &out_grad,
+                                   const at::Tensor &input,
+                                   const at::Tensor &bbox,
+                                   const at::Tensor &trans,
+                                   const at::Tensor &top_count,
+                                   const int no_trans,
+                                   const float spatial_scale,
+                                   const int output_dim,
+                                   const int group_size,
+                                   const int pooled_size,
+                                   const int part_size,
+                                   const int sample_per_part,
+                                   const float trans_std);
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_cuda.cu b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_cuda.cu
new file mode 100644
index 0000000..5e693f4
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_cuda.cu
@@ -0,0 +1,336 @@
+#include <vector>
+#include "cuda/dcn_v2_im2col_cuda.h"
+
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+
+#include <THC/THC.h>
+#include <THC/THCAtomics.cuh>
+#include <THC/THCDeviceUtils.cuh>
+
+//extern THCState *state;
+THCState *state = at::globalContext().lazyInitCUDA();
+
+// author: Charles Shang
+// https://github.com/torch/cunn/blob/master/lib/THCUNN/generic/SpatialConvolutionMM.cu
+
+// [batch gemm]
+// https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorMathBlas.cu
+
+__global__ void createBatchGemmBuffer(const float **input_b, float **output_b,
+                                      float **columns_b, const float **ones_b,
+                                      const float **weight_b, const float **bias_b,
+                                      float *input, float *output,
+                                      float *columns, float *ones,
+                                      float *weight, float *bias,
+                                      const int input_stride, const int output_stride,
+                                      const int columns_stride, const int ones_stride,
+                                      const int num_batches)
+{
+    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
+    if (idx < num_batches)
+    {
+        input_b[idx] = input + idx * input_stride;
+        output_b[idx] = output + idx * output_stride;
+        columns_b[idx] = columns + idx * columns_stride;
+        ones_b[idx] = ones + idx * ones_stride;
+        // share weights and bias within a Mini-Batch
+        weight_b[idx] = weight;
+        bias_b[idx] = bias;
+    }
+}
+
+at::Tensor
+dcn_v2_cuda_forward(const at::Tensor &input,
+                    const at::Tensor &weight,
+                    const at::Tensor &bias,
+                    const at::Tensor &offset,
+                    const at::Tensor &mask,
+                    const int kernel_h,
+                    const int kernel_w,
+                    const int stride_h,
+                    const int stride_w,
+                    const int pad_h,
+                    const int pad_w,
+                    const int dilation_h,
+                    const int dilation_w,
+                    const int deformable_group)
+{
+    using scalar_t = float;
+    // THCAssertSameGPU(THCudaTensor_checkGPU(state, 5, input, weight, bias, offset, mask));
+    AT_ASSERTM(input.type().is_cuda(), "input must be a CUDA tensor");
+    AT_ASSERTM(weight.type().is_cuda(), "weight must be a CUDA tensor");
+    AT_ASSERTM(bias.type().is_cuda(), "bias must be a CUDA tensor");
+    AT_ASSERTM(offset.type().is_cuda(), "offset must be a CUDA tensor");
+    AT_ASSERTM(mask.type().is_cuda(), "mask must be a CUDA tensor");
+
+    const int batch = input.size(0);
+    const int channels = input.size(1);
+    const int height = input.size(2);
+    const int width = input.size(3);
+
+    const int channels_out = weight.size(0);
+    const int channels_kernel = weight.size(1);
+    const int kernel_h_ = weight.size(2);
+    const int kernel_w_ = weight.size(3);
+
+    // printf("Kernels: %d %d %d %d\n", kernel_h_, kernel_w_, kernel_w, kernel_h);
+    // printf("Channels: %d %d\n", channels, channels_kernel);
+    // printf("Channels: %d %d\n", channels_out, channels_kernel);
+
+    AT_ASSERTM(kernel_h_ == kernel_h && kernel_w_ == kernel_w,
+               "Input shape and kernel shape wont match: (%d x %d vs %d x %d).", kernel_h_, kernel_w, kernel_h_, kernel_w_);
+
+    AT_ASSERTM(channels == channels_kernel,
+               "Input shape and kernel channels wont match: (%d vs %d).", channels, channels_kernel);
+
+    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
+    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
+
+    auto ones = at::ones({batch, height_out, width_out}, input.options());
+    auto columns = at::empty({batch, channels * kernel_h * kernel_w, 1 * height_out * width_out}, input.options());
+    auto output = at::empty({batch, channels_out, height_out, width_out}, input.options());
+
+    // prepare for batch-wise computing, which is significantly faster than instance-wise computing
+    // when batch size is large.
+    // launch batch threads
+    int matrices_size = batch * sizeof(float *);
+    auto input_b = static_cast<const float **>(THCudaMalloc(state, matrices_size));
+    auto output_b = static_cast<float **>(THCudaMalloc(state, matrices_size));
+    auto columns_b = static_cast<float **>(THCudaMalloc(state, matrices_size));
+    auto ones_b = static_cast<const float **>(THCudaMalloc(state, matrices_size));
+    auto weight_b = static_cast<const float **>(THCudaMalloc(state, matrices_size));
+    auto bias_b = static_cast<const float **>(THCudaMalloc(state, matrices_size));
+
+    const int block = 128;
+    const int grid = (batch + block - 1) / block;
+
+    createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+        input_b, output_b,
+        columns_b, ones_b,
+        weight_b, bias_b,
+        input.data<scalar_t>(),
+        output.data<scalar_t>(),
+        columns.data<scalar_t>(),
+        ones.data<scalar_t>(),
+        weight.data<scalar_t>(),
+        bias.data<scalar_t>(),
+        channels * width * height,
+        channels_out * width_out * height_out,
+        channels * kernel_h * kernel_w * height_out * width_out,
+        height_out * width_out,
+        batch);
+
+    long m_ = channels_out;
+    long n_ = height_out * width_out;
+    long k_ = 1;
+    THCudaBlas_SgemmBatched(state,
+                            't',
+                            'n',
+                            n_,
+                            m_,
+                            k_,
+                            1.0f,
+                            ones_b, k_,
+                            bias_b, k_,
+                            0.0f,
+                            output_b, n_,
+                            batch);
+
+    modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
+                                     input.data<scalar_t>(),
+                                     offset.data<scalar_t>(),
+                                     mask.data<scalar_t>(),
+                                     batch, channels, height, width,
+                                     height_out, width_out, kernel_h, kernel_w,
+                                     pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,
+                                     deformable_group,
+                                     columns.data<scalar_t>());
+
+    long m = channels_out;
+    long n = height_out * width_out;
+    long k = channels * kernel_h * kernel_w;
+    THCudaBlas_SgemmBatched(state,
+                            'n',
+                            'n',
+                            n,
+                            m,
+                            k,
+                            1.0f,
+                            (const float **)columns_b, n,
+                            weight_b, k,
+                            1.0f,
+                            output_b, n,
+                            batch);
+
+    THCudaFree(state, input_b);
+    THCudaFree(state, output_b);
+    THCudaFree(state, columns_b);
+    THCudaFree(state, ones_b);
+    THCudaFree(state, weight_b);
+    THCudaFree(state, bias_b);
+    return output;
+}
+
+__global__ void createBatchGemmBufferBackward(
+    float **grad_output_b,
+    float **columns_b,
+    float **ones_b,
+    float **weight_b,
+    float **grad_weight_b,
+    float **grad_bias_b,
+    float *grad_output,
+    float *columns,
+    float *ones,
+    float *weight,
+    float *grad_weight,
+    float *grad_bias,
+    const int grad_output_stride,
+    const int columns_stride,
+    const int ones_stride,
+    const int num_batches)
+{
+    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
+    if (idx < num_batches)
+    {
+        grad_output_b[idx] = grad_output + idx * grad_output_stride;
+        columns_b[idx] = columns + idx * columns_stride;
+        ones_b[idx] = ones + idx * ones_stride;
+
+        // share weights and bias within a Mini-Batch
+        weight_b[idx] = weight;
+        grad_weight_b[idx] = grad_weight;
+        grad_bias_b[idx] = grad_bias;
+    }
+}
+
+std::vector<at::Tensor> dcn_v2_cuda_backward(const at::Tensor &input,
+                                             const at::Tensor &weight,
+                                             const at::Tensor &bias,
+                                             const at::Tensor &offset,
+                                             const at::Tensor &mask,
+                                             const at::Tensor &grad_output,
+                                             int kernel_h, int kernel_w,
+                                             int stride_h, int stride_w,
+                                             int pad_h, int pad_w,
+                                             int dilation_h, int dilation_w,
+                                             int deformable_group)
+{
+
+    THArgCheck(input.is_contiguous(), 1, "input tensor has to be contiguous");
+    THArgCheck(weight.is_contiguous(), 2, "weight tensor has to be contiguous");
+
+    AT_ASSERTM(input.type().is_cuda(), "input must be a CUDA tensor");
+    AT_ASSERTM(weight.type().is_cuda(), "weight must be a CUDA tensor");
+    AT_ASSERTM(bias.type().is_cuda(), "bias must be a CUDA tensor");
+    AT_ASSERTM(offset.type().is_cuda(), "offset must be a CUDA tensor");
+    AT_ASSERTM(mask.type().is_cuda(), "mask must be a CUDA tensor");
+
+    const int batch = input.size(0);
+    const int channels = input.size(1);
+    const int height = input.size(2);
+    const int width = input.size(3);
+
+    const int channels_out = weight.size(0);
+    const int channels_kernel = weight.size(1);
+    const int kernel_h_ = weight.size(2);
+    const int kernel_w_ = weight.size(3);
+
+    AT_ASSERTM(kernel_h_ == kernel_h && kernel_w_ == kernel_w,
+               "Input shape and kernel shape wont match: (%d x %d vs %d x %d).", kernel_h_, kernel_w, kernel_h_, kernel_w_);
+
+    AT_ASSERTM(channels == channels_kernel,
+               "Input shape and kernel channels wont match: (%d vs %d).", channels, channels_kernel);
+
+    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
+    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
+
+    auto ones = at::ones({height_out, width_out}, input.options());
+    auto columns = at::empty({channels * kernel_h * kernel_w, 1 * height_out * width_out}, input.options());
+    auto output = at::empty({batch, channels_out, height_out, width_out}, input.options());
+
+    auto grad_input = at::zeros_like(input);
+    auto grad_weight = at::zeros_like(weight);
+    auto grad_bias = at::zeros_like(bias);
+    auto grad_offset = at::zeros_like(offset);
+    auto grad_mask = at::zeros_like(mask);
+
+    using scalar_t = float;
+
+    for (int b = 0; b < batch; b++)
+    {
+        auto input_n = input.select(0, b);
+        auto offset_n = offset.select(0, b);
+        auto mask_n = mask.select(0, b);
+        auto grad_output_n = grad_output.select(0, b);
+        auto grad_input_n = grad_input.select(0, b);
+        auto grad_offset_n = grad_offset.select(0, b);
+        auto grad_mask_n = grad_mask.select(0, b);
+
+        long m = channels * kernel_h * kernel_w;
+        long n = height_out * width_out;
+        long k = channels_out;
+
+        THCudaBlas_Sgemm(state, 'n', 't', n, m, k, 1.0f,
+                         grad_output_n.data<scalar_t>(), n,
+                         weight.data<scalar_t>(), m, 0.0f,
+                         columns.data<scalar_t>(), n);
+
+        // gradient w.r.t. input coordinate data
+        modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),
+                                               columns.data<scalar_t>(),
+                                               input_n.data<scalar_t>(),
+                                               offset_n.data<scalar_t>(),
+                                               mask_n.data<scalar_t>(),
+                                               1, channels, height, width,
+                                               height_out, width_out, kernel_h, kernel_w,
+                                               pad_h, pad_w, stride_h, stride_w,
+                                               dilation_h, dilation_w, deformable_group,
+                                               grad_offset_n.data<scalar_t>(),
+                                               grad_mask_n.data<scalar_t>());
+        // gradient w.r.t. input data
+        modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),
+                                         columns.data<scalar_t>(),
+                                         offset_n.data<scalar_t>(),
+                                         mask_n.data<scalar_t>(),
+                                         1, channels, height, width,
+                                         height_out, width_out, kernel_h, kernel_w,
+                                         pad_h, pad_w, stride_h, stride_w,
+                                         dilation_h, dilation_w, deformable_group,
+                                         grad_input_n.data<scalar_t>());
+
+        // gradient w.r.t. weight, dWeight should accumulate across the batch and group
+        modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
+                                         input_n.data<scalar_t>(),
+                                         offset_n.data<scalar_t>(),
+                                         mask_n.data<scalar_t>(),
+                                         1, channels, height, width,
+                                         height_out, width_out, kernel_h, kernel_w,
+                                         pad_h, pad_w, stride_h, stride_w,
+                                         dilation_h, dilation_w, deformable_group,
+                                         columns.data<scalar_t>());
+
+        long m_ = channels_out;
+        long n_ = channels * kernel_h * kernel_w;
+        long k_ = height_out * width_out;
+
+        THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,
+                         columns.data<scalar_t>(), k_,
+                         grad_output_n.data<scalar_t>(), k_, 1.0f,
+                         grad_weight.data<scalar_t>(), n_);
+
+        // gradient w.r.t. bias
+        // long m_ = channels_out;
+        // long k__ = height_out * width_out;
+        THCudaBlas_Sgemv(state,
+                         't',
+                         k_, m_, 1.0f,
+                         grad_output_n.data<scalar_t>(), k_,
+                         ones.data<scalar_t>(), 1, 1.0f,
+                         grad_bias.data<scalar_t>(), 1);
+    }
+
+    return {
+        grad_input, grad_offset, grad_mask, grad_weight, grad_bias
+    };
+}
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.cu b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.cu
index ab22b1b..4183793 100644
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.cu
+++ b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.cu
@@ -3,6 +3,13 @@
 #include <algorithm>
 #include <cstring>
 
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+
+#include <THC/THC.h>
+#include <THC/THCAtomics.cuh>
+#include <THC/THCDeviceUtils.cuh>
+
 #define CUDA_KERNEL_LOOP(i, n)                          \
   for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \
       i < (n);                                          \
@@ -126,13 +133,20 @@ __global__ void modulated_deformable_im2col_gpu_kernel(const int n,
                                                        const int height_col, const int width_col,
                                                        float *data_col)
 {
+  // launch channels * batch_size * height_col * width_col cores
   CUDA_KERNEL_LOOP(index, n)
   {
+    // NOTE(CharlesShang): different from Dai Jifeng's MXNet implementation, col_buffer is of shape (c*kw*kh, N, oh, ow)
+    // here columns is of shape (N, c*kw*kh, oh * ow), need to adapt axis
+
     // index index of output matrix
     const int w_col = index % width_col;
     const int h_col = (index / width_col) % height_col;
-    const int b_col = (index / width_col / height_col) % batch_size;
-    const int c_im = (index / width_col / height_col) / batch_size;
+    // const int b_col = (index / width_col / height_col) % batch_size;
+    const int b_col = (index / width_col / height_col / num_channels) % batch_size;
+    // const int c_im = (index / width_col / height_col) / batch_size;
+    const int c_im = (index / width_col / height_col) % num_channels;
+    // const int c_col = c_im * kernel_h * kernel_w;
     const int c_col = c_im * kernel_h * kernel_w;
 
     // compute deformable group index
@@ -141,7 +155,8 @@ __global__ void modulated_deformable_im2col_gpu_kernel(const int n,
     const int h_in = h_col * stride_h - pad_h;
     const int w_in = w_col * stride_w - pad_w;
 
-    float *data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
+    //  float *data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
+    float *data_col_ptr = data_col + ((b_col * num_channels * kernel_w * kernel_h + c_col) * height_col + h_col) * width_col + w_col;
     //const float* data_im_ptr = data_im + ((b_col * num_channels + c_im) * height + h_in) * width + w_in;
     const float *data_im_ptr = data_im + (b_col * num_channels + c_im) * height * width;
     const float *data_offset_ptr = data_offset + (b_col * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;
@@ -172,8 +187,8 @@ __global__ void modulated_deformable_im2col_gpu_kernel(const int n,
           val = dmcn_im2col_bilinear(data_im_ptr, width, height, width, h_im, w_im);
         }
         *data_col_ptr = val * mask;
-        data_col_ptr += batch_size * height_col * width_col;
-        //data_col_ptr += height_col * width_col;
+        // data_col_ptr += batch_size * height_col * width_col;
+        data_col_ptr += height_col * width_col;
       }
     }
   }
@@ -314,7 +329,7 @@ __global__ void modulated_deformable_col2im_coord_gpu_kernel(const int n,
 void modulated_deformable_im2col_cuda(cudaStream_t stream,
   const float* data_im, const float* data_offset, const float* data_mask,
   const int batch_size, const int channels, const int height_im, const int width_im, 
-  const int height_col, const int width_col, const int kernel_h, const int kenerl_w,
+  const int height_col, const int width_col, const int kernel_h, const int kernel_w,
   const int pad_h, const int pad_w, const int stride_h, const int stride_w, 
   const int dilation_h, const int dilation_w,
   const int deformable_group, float* data_col) {
@@ -324,7 +339,7 @@ void modulated_deformable_im2col_cuda(cudaStream_t stream,
   modulated_deformable_im2col_gpu_kernel
       <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS,
           0, stream>>>(
-      num_kernels, data_im, data_offset, data_mask, height_im, width_im, kernel_h, kenerl_w,
+      num_kernels, data_im, data_offset, data_mask, height_im, width_im, kernel_h, kernel_w,
       pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w, channel_per_deformable_group,
       batch_size, channels, deformable_group, height_col, width_col, data_col);
   
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.h b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.h
index 3457e96..c856831 100644
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.h
+++ b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.h
@@ -1,3 +1,4 @@
+
 /*!
  ******************* BEGIN Caffe Copyright Notice and Disclaimer ****************
  *
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.cu b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.cu
deleted file mode 100644
index 29cb048..0000000
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.cu
+++ /dev/null
@@ -1,399 +0,0 @@
-#include "dcn_v2_im2col_cuda_double.h"
-#include <cstdio>
-#include <algorithm>
-#include <cstring>
-
-#define CUDA_KERNEL_LOOP(i, n)                        \
-  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
-       i < (n);                                       \
-       i += blockDim.x * gridDim.x)
-
-const int CUDA_NUM_THREADS = 512;
-inline int GET_BLOCKS(const int N)
-{
-  return (N + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS;
-}
-
-#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 600
-#else
-__device__ double atomicAdd(double* address, double val)
-{
-    unsigned long long int* address_as_ull = (unsigned long long int*)address;
-    unsigned long long int old = *address_as_ull, assumed;
-    do {
-        assumed = old;
-        old = atomicCAS(address_as_ull, assumed,
-                __double_as_longlong(val + __longlong_as_double(assumed)));
-    } while (assumed != old);
-    return __longlong_as_double(old);
-}
-#endif
-
-__device__ double dmcn_im2col_bilinear(const double *bottom_data, const int data_width,
-                                       const int height, const int width, double h, double w)
-{
-  int h_low = floor(h);
-  int w_low = floor(w);
-  int h_high = h_low + 1;
-  int w_high = w_low + 1;
-
-  double lh = h - h_low;
-  double lw = w - w_low;
-  double hh = 1 - lh, hw = 1 - lw;
-
-  double v1 = 0;
-  if (h_low >= 0 && w_low >= 0)
-    v1 = bottom_data[h_low * data_width + w_low];
-  double v2 = 0;
-  if (h_low >= 0 && w_high <= width - 1)
-    v2 = bottom_data[h_low * data_width + w_high];
-  double v3 = 0;
-  if (h_high <= height - 1 && w_low >= 0)
-    v3 = bottom_data[h_high * data_width + w_low];
-  double v4 = 0;
-  if (h_high <= height - 1 && w_high <= width - 1)
-    v4 = bottom_data[h_high * data_width + w_high];
-
-  double w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;
-
-  double val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
-  return val;
-}
-
-__device__ double dmcn_get_gradient_weight(double argmax_h, double argmax_w,
-                                           const int h, const int w, const int height, const int width)
-{
-  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 || argmax_w >= width)
-  {
-    //empty
-    return 0;
-  }
-
-  int argmax_h_low = floor(argmax_h);
-  int argmax_w_low = floor(argmax_w);
-  int argmax_h_high = argmax_h_low + 1;
-  int argmax_w_high = argmax_w_low + 1;
-
-  double weight = 0;
-  if (h == argmax_h_low && w == argmax_w_low)
-    weight = (h + 1 - argmax_h) * (w + 1 - argmax_w);
-  if (h == argmax_h_low && w == argmax_w_high)
-    weight = (h + 1 - argmax_h) * (argmax_w + 1 - w);
-  if (h == argmax_h_high && w == argmax_w_low)
-    weight = (argmax_h + 1 - h) * (w + 1 - argmax_w);
-  if (h == argmax_h_high && w == argmax_w_high)
-    weight = (argmax_h + 1 - h) * (argmax_w + 1 - w);
-  return weight;
-}
-
-__device__ double dmcn_get_coordinate_weight(double argmax_h, double argmax_w,
-                                             const int height, const int width, const double *im_data,
-                                             const int data_width, const int bp_dir)
-{
-  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 || argmax_w >= width)
-  {
-    //empty
-    return 0;
-  }
-
-  int argmax_h_low = floor(argmax_h);
-  int argmax_w_low = floor(argmax_w);
-  int argmax_h_high = argmax_h_low + 1;
-  int argmax_w_high = argmax_w_low + 1;
-
-  double weight = 0;
-
-  if (bp_dir == 0)
-  {
-    if (argmax_h_low >= 0 && argmax_w_low >= 0)
-      weight += -1 * (argmax_w_low + 1 - argmax_w) * im_data[argmax_h_low * data_width + argmax_w_low];
-    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
-      weight += -1 * (argmax_w - argmax_w_low) * im_data[argmax_h_low * data_width + argmax_w_high];
-    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
-      weight += (argmax_w_low + 1 - argmax_w) * im_data[argmax_h_high * data_width + argmax_w_low];
-    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
-      weight += (argmax_w - argmax_w_low) * im_data[argmax_h_high * data_width + argmax_w_high];
-  }
-  else if (bp_dir == 1)
-  {
-    if (argmax_h_low >= 0 && argmax_w_low >= 0)
-      weight += -1 * (argmax_h_low + 1 - argmax_h) * im_data[argmax_h_low * data_width + argmax_w_low];
-    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
-      weight += (argmax_h_low + 1 - argmax_h) * im_data[argmax_h_low * data_width + argmax_w_high];
-    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
-      weight += -1 * (argmax_h - argmax_h_low) * im_data[argmax_h_high * data_width + argmax_w_low];
-    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
-      weight += (argmax_h - argmax_h_low) * im_data[argmax_h_high * data_width + argmax_w_high];
-  }
-
-  return weight;
-}
-
-__global__ void modulated_deformable_im2col_gpu_kernel(const int n,
-                                                       const double *data_im, const double *data_offset, const double *data_mask,
-                                                       const int height, const int width, const int kernel_h, const int kernel_w,
-                                                       const int pad_h, const int pad_w,
-                                                       const int stride_h, const int stride_w,
-                                                       const int dilation_h, const int dilation_w,
-                                                       const int channel_per_deformable_group,
-                                                       const int batch_size, const int num_channels, const int deformable_group,
-                                                       const int height_col, const int width_col,
-                                                       double *data_col)
-{
-  CUDA_KERNEL_LOOP(index, n)
-  {
-    // index index of output matrix
-    const int w_col = index % width_col;
-    const int h_col = (index / width_col) % height_col;
-    const int b_col = (index / width_col / height_col) % batch_size;
-    const int c_im = (index / width_col / height_col) / batch_size;
-    const int c_col = c_im * kernel_h * kernel_w;
-
-    // compute deformable group index
-    const int deformable_group_index = c_im / channel_per_deformable_group;
-
-    const int h_in = h_col * stride_h - pad_h;
-    const int w_in = w_col * stride_w - pad_w;
-
-    double *data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
-    //const double* data_im_ptr = data_im + ((b_col * num_channels + c_im) * height + h_in) * width + w_in;
-    const double *data_im_ptr = data_im + (b_col * num_channels + c_im) * height * width;
-    const double *data_offset_ptr = data_offset + (b_col * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;
-
-    const double *data_mask_ptr = data_mask + (b_col * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;
-
-    for (int i = 0; i < kernel_h; ++i)
-    {
-      for (int j = 0; j < kernel_w; ++j)
-      {
-        const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;
-        const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col + w_col;
-        const int data_mask_hw_ptr = ((i * kernel_w + j) * height_col + h_col) * width_col + w_col;
-        const double offset_h = data_offset_ptr[data_offset_h_ptr];
-        const double offset_w = data_offset_ptr[data_offset_w_ptr];
-        const double mask = data_mask_ptr[data_mask_hw_ptr];
-        double val = static_cast<double>(0);
-        const double h_im = h_in + i * dilation_h + offset_h;
-        const double w_im = w_in + j * dilation_w + offset_w;
-        //if (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) {
-        if (h_im > -1 && w_im > -1 && h_im < height && w_im < width)
-        {
-          //const double map_h = i * dilation_h + offset_h;
-          //const double map_w = j * dilation_w + offset_w;
-          //const int cur_height = height - h_in;
-          //const int cur_width = width - w_in;
-          //val = dmcn_im2col_bilinear(data_im_ptr, width, cur_height, cur_width, map_h, map_w);
-          val = dmcn_im2col_bilinear(data_im_ptr, width, height, width, h_im, w_im);
-        }
-        *data_col_ptr = val * mask;
-        data_col_ptr += batch_size * height_col * width_col;
-        //data_col_ptr += height_col * width_col;
-      }
-    }
-  }
-}
-
-__global__ void modulated_deformable_col2im_gpu_kernel(const int n,
-                                                       const double *data_col, const double *data_offset, const double *data_mask,
-                                                       const int channels, const int height, const int width,
-                                                       const int kernel_h, const int kernel_w,
-                                                       const int pad_h, const int pad_w,
-                                                       const int stride_h, const int stride_w,
-                                                       const int dilation_h, const int dilation_w,
-                                                       const int channel_per_deformable_group,
-                                                       const int batch_size, const int deformable_group,
-                                                       const int height_col, const int width_col,
-                                                       double *grad_im)
-{
-  CUDA_KERNEL_LOOP(index, n)
-  {
-    const int j = (index / width_col / height_col / batch_size) % kernel_w;
-    const int i = (index / width_col / height_col / batch_size / kernel_w) % kernel_h;
-    const int c = index / width_col / height_col / batch_size / kernel_w / kernel_h;
-    // compute the start and end of the output
-
-    const int deformable_group_index = c / channel_per_deformable_group;
-
-    int w_out = index % width_col;
-    int h_out = (index / width_col) % height_col;
-    int b = (index / width_col / height_col) % batch_size;
-    int w_in = w_out * stride_w - pad_w;
-    int h_in = h_out * stride_h - pad_h;
-
-    const double *data_offset_ptr = data_offset + (b * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;
-    const double *data_mask_ptr = data_mask + (b * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;
-    const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out;
-    const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out;
-    const int data_mask_hw_ptr = ((i * kernel_w + j) * height_col + h_out) * width_col + w_out;
-    const double offset_h = data_offset_ptr[data_offset_h_ptr];
-    const double offset_w = data_offset_ptr[data_offset_w_ptr];
-    const double mask = data_mask_ptr[data_mask_hw_ptr];
-    const double cur_inv_h_data = h_in + i * dilation_h + offset_h;
-    const double cur_inv_w_data = w_in + j * dilation_w + offset_w;
-
-    const double cur_top_grad = data_col[index] * mask;
-    const int cur_h = (int)cur_inv_h_data;
-    const int cur_w = (int)cur_inv_w_data;
-    for (int dy = -2; dy <= 2; dy++)
-    {
-      for (int dx = -2; dx <= 2; dx++)
-      {
-        if (cur_h + dy >= 0 && cur_h + dy < height &&
-            cur_w + dx >= 0 && cur_w + dx < width &&
-            abs(cur_inv_h_data - (cur_h + dy)) < 1 &&
-            abs(cur_inv_w_data - (cur_w + dx)) < 1)
-        {
-          int cur_bottom_grad_pos = ((b * channels + c) * height + cur_h + dy) * width + cur_w + dx;
-          double weight = dmcn_get_gradient_weight(cur_inv_h_data, cur_inv_w_data, cur_h + dy, cur_w + dx, height, width);
-          atomicAdd(grad_im + cur_bottom_grad_pos, weight * cur_top_grad);
-        }
-      }
-    }
-  }
-}
-
-__global__ void modulated_deformable_col2im_coord_gpu_kernel(const int n,
-                                                             const double *data_col, const double *data_im,
-                                                             const double *data_offset, const double *data_mask,
-                                                             const int channels, const int height, const int width,
-                                                             const int kernel_h, const int kernel_w,
-                                                             const int pad_h, const int pad_w,
-                                                             const int stride_h, const int stride_w,
-                                                             const int dilation_h, const int dilation_w,
-                                                             const int channel_per_deformable_group,
-                                                             const int batch_size, const int offset_channels, const int deformable_group,
-                                                             const int height_col, const int width_col,
-                                                             double *grad_offset, double *grad_mask)
-{
-  CUDA_KERNEL_LOOP(index, n)
-  {
-    double val = 0, mval = 0;
-    int w = index % width_col;
-    int h = (index / width_col) % height_col;
-    int c = (index / width_col / height_col) % offset_channels;
-    int b = (index / width_col / height_col) / offset_channels;
-    // compute the start and end of the output
-
-    const int deformable_group_index = c / (2 * kernel_h * kernel_w);
-    const int col_step = kernel_h * kernel_w;
-    int cnt = 0;
-    const double *data_col_ptr = data_col + deformable_group_index * channel_per_deformable_group * batch_size * width_col * height_col;
-    const double *data_im_ptr = data_im + (b * deformable_group + deformable_group_index) * channel_per_deformable_group / kernel_h / kernel_w * height * width;
-    const double *data_offset_ptr = data_offset + (b * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;
-    const double *data_mask_ptr = data_mask + (b * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;
-
-    const int offset_c = c - deformable_group_index * 2 * kernel_h * kernel_w;
-
-    for (int col_c = (offset_c / 2); col_c < channel_per_deformable_group; col_c += col_step)
-    {
-      const int col_pos = (((col_c * batch_size + b) * height_col) + h) * width_col + w;
-      const int bp_dir = offset_c % 2;
-
-      int j = (col_pos / width_col / height_col / batch_size) % kernel_w;
-      int i = (col_pos / width_col / height_col / batch_size / kernel_w) % kernel_h;
-      int w_out = col_pos % width_col;
-      int h_out = (col_pos / width_col) % height_col;
-      int w_in = w_out * stride_w - pad_w;
-      int h_in = h_out * stride_h - pad_h;
-      const int data_offset_h_ptr = (((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out);
-      const int data_offset_w_ptr = (((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out);
-      const int data_mask_hw_ptr = (((i * kernel_w + j) * height_col + h_out) * width_col + w_out);
-      const double offset_h = data_offset_ptr[data_offset_h_ptr];
-      const double offset_w = data_offset_ptr[data_offset_w_ptr];
-      const double mask = data_mask_ptr[data_mask_hw_ptr];
-      double inv_h = h_in + i * dilation_h + offset_h;
-      double inv_w = w_in + j * dilation_w + offset_w;
-      if (inv_h <= -1 || inv_w <= -1 || inv_h >= height || inv_w >= width)
-      {
-        inv_h = inv_w = -2;
-      }
-      else
-      {
-        mval += data_col_ptr[col_pos] * dmcn_im2col_bilinear(data_im_ptr + cnt * height * width, width, height, width, inv_h, inv_w);
-      }
-      const double weight = dmcn_get_coordinate_weight(
-          inv_h, inv_w,
-          height, width, data_im_ptr + cnt * height * width, width, bp_dir);
-      val += weight * data_col_ptr[col_pos] * mask;
-      cnt += 1;
-    }
-    // KERNEL_ASSIGN(grad_offset[index], offset_req, val);
-    grad_offset[index] = val;
-    if (offset_c % 2 == 0)
-      // KERNEL_ASSIGN(grad_mask[(((b * deformable_group + deformable_group_index) * kernel_h * kernel_w + offset_c / 2) * height_col + h) * width_col + w], mask_req, mval);
-      grad_mask[(((b * deformable_group + deformable_group_index) * kernel_h * kernel_w + offset_c / 2) * height_col + h) * width_col + w] = mval;
-  }
-}
-
-void modulated_deformable_im2col_cuda(cudaStream_t stream,
-                                      const double *data_im, const double *data_offset, const double *data_mask,
-                                      const int batch_size, const int channels, const int height_im, const int width_im,
-                                      const int height_col, const int width_col, const int kernel_h, const int kenerl_w,
-                                      const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                      const int dilation_h, const int dilation_w,
-                                      const int deformable_group, double *data_col)
-{
-  // num_axes should be smaller than block size
-  const int channel_per_deformable_group = channels / deformable_group;
-  const int num_kernels = channels * batch_size * height_col * width_col;
-  modulated_deformable_im2col_gpu_kernel<<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS,
-                                           0, stream>>>(
-      num_kernels, data_im, data_offset, data_mask, height_im, width_im, kernel_h, kenerl_w,
-      pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w, channel_per_deformable_group,
-      batch_size, channels, deformable_group, height_col, width_col, data_col);
-
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
-  {
-    printf("error in modulated_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-void modulated_deformable_col2im_cuda(cudaStream_t stream,
-                                      const double *data_col, const double *data_offset, const double *data_mask,
-                                      const int batch_size, const int channels, const int height_im, const int width_im,
-                                      const int height_col, const int width_col, const int kernel_h, const int kernel_w,
-                                      const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                      const int dilation_h, const int dilation_w,
-                                      const int deformable_group, double *grad_im)
-{
-
-  const int channel_per_deformable_group = channels / deformable_group;
-  const int num_kernels = channels * kernel_h * kernel_w * batch_size * height_col * width_col;
-  modulated_deformable_col2im_gpu_kernel<<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS,
-                                           0, stream>>>(
-      num_kernels, data_col, data_offset, data_mask, channels, height_im, width_im,
-      kernel_h, kernel_w, pad_h, pad_h, stride_h, stride_w,
-      dilation_h, dilation_w, channel_per_deformable_group,
-      batch_size, deformable_group, height_col, width_col, grad_im);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
-  {
-    printf("error in modulated_deformable_col2im_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-void modulated_deformable_col2im_coord_cuda(cudaStream_t stream,
-                                            const double *data_col, const double *data_im, const double *data_offset, const double *data_mask,
-                                            const int batch_size, const int channels, const int height_im, const int width_im,
-                                            const int height_col, const int width_col, const int kernel_h, const int kernel_w,
-                                            const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                            const int dilation_h, const int dilation_w,
-                                            const int deformable_group,
-                                            double *grad_offset, double *grad_mask)
-{
-  const int num_kernels = batch_size * height_col * width_col * 2 * kernel_h * kernel_w * deformable_group;
-  const int channel_per_deformable_group = channels * kernel_h * kernel_w / deformable_group;
-  modulated_deformable_col2im_coord_gpu_kernel<<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS,
-                                                 0, stream>>>(
-      num_kernels, data_col, data_im, data_offset, data_mask, channels, height_im, width_im,
-      kernel_h, kernel_w, pad_h, pad_w, stride_h, stride_w,
-      dilation_h, dilation_w, channel_per_deformable_group,
-      batch_size, 2 * kernel_h * kernel_w * deformable_group, deformable_group, height_col, width_col,
-      grad_offset, grad_mask);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
-  {
-    printf("error in modulated_deformable_col2im_coord_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.h b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.h
deleted file mode 100644
index a461692..0000000
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda_double.h
+++ /dev/null
@@ -1,100 +0,0 @@
-/*!
- ******************* BEGIN Caffe Copyright Notice and Disclaimer ****************
- *
- * COPYRIGHT
- *
- * All contributions by the University of California:
- * Copyright (c) 2014-2017 The Regents of the University of California (Regents)
- * All rights reserved.
- *
- * All other contributions:
- * Copyright (c) 2014-2017, the respective contributors
- * All rights reserved.
- *
- * Caffe uses a shared copyright model: each contributor holds copyright over
- * their contributions to Caffe. The project versioning records all such
- * contribution and copyright details. If a contributor wants to further mark
- * their specific copyright on a particular contribution, they should indicate
- * their copyright solely in the commit message of the change when it is
- * committed.
- *
- * LICENSE
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice, this
- * list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
- * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- * CONTRIBUTION AGREEMENT
- *
- * By contributing to the BVLC/caffe repository through pull-request, comment,
- * or otherwise, the contributor releases their content to the
- * license and copyright terms herein.
- *
- ***************** END Caffe Copyright Notice and Disclaimer ********************
- *
- * Copyright (c) 2018 Microsoft
- * Licensed under The MIT License [see LICENSE for details]
- * \file modulated_deformable_im2col.h
- * \brief Function definitions of converting an image to
- * column matrix based on kernel, padding, dilation, and offset.
- * These functions are mainly used in deformable convolution operators.
- * \ref: https://arxiv.org/abs/1811.11168
- * \author Yuwen Xiong, Haozhi Qi, Jifeng Dai, Xizhou Zhu, Han Hu
- */
-
-/***************** Adapted by Charles Shang *********************/
-
-#ifndef DCN_V2_IM2COL_CUDA_DOUBLE
-#define DCN_V2_IM2COL_CUDA_DOUBLE
-
-#ifdef __cplusplus
-extern "C"
-{
-#endif
-
-  void modulated_deformable_im2col_cuda(cudaStream_t stream,
-                                        const double *data_im, const double *data_offset, const double *data_mask,
-                                        const int batch_size, const int channels, const int height_im, const int width_im,
-                                        const int height_col, const int width_col, const int kernel_h, const int kenerl_w,
-                                        const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                        const int dilation_h, const int dilation_w,
-                                        const int deformable_group, double *data_col);
-
-  void modulated_deformable_col2im_cuda(cudaStream_t stream,
-                                        const double *data_col, const double *data_offset, const double *data_mask,
-                                        const int batch_size, const int channels, const int height_im, const int width_im,
-                                        const int height_col, const int width_col, const int kernel_h, const int kenerl_w,
-                                        const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                        const int dilation_h, const int dilation_w,
-                                        const int deformable_group, double *grad_im);
-
-  void modulated_deformable_col2im_coord_cuda(cudaStream_t stream,
-                                         const double *data_col, const double *data_im, const double *data_offset, const double *data_mask,
-                                         const int batch_size, const int channels, const int height_im, const int width_im,
-                                         const int height_col, const int width_col, const int kernel_h, const int kenerl_w,
-                                         const int pad_h, const int pad_w, const int stride_h, const int stride_w,
-                                         const int dilation_h, const int dilation_w,
-                                         const int deformable_group,
-                                         double *grad_offset, double *grad_mask);
-
-#ifdef __cplusplus
-}
-#endif
-
-#endif
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu
index 295657c..07b438e 100644
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu
+++ b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu
@@ -6,10 +6,18 @@
  * \author Yi Li, Guodong Zhang, Jifeng Dai
 */
 /***************** Adapted by Charles Shang *********************/
-#include "dcn_v2_psroi_pooling_cuda.h"
+
 #include <cstdio>
 #include <algorithm>
 #include <cstring>
+#include <iostream>
+
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+
+#include <THC/THC.h>
+#include <THC/THCAtomics.cuh>
+#include <THC/THCDeviceUtils.cuh>
 
 #define CUDA_KERNEL_LOOP(i, n)                        \
   for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
@@ -22,10 +30,11 @@ inline int GET_BLOCKS(const int N)
   return (N + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS;
 }
 
-__device__ float bilinear_interp(
-    const float *data,
-    const float x,
-    const float y,
+template <typename T>
+__device__ T bilinear_interp(
+    const T *data,
+    const T x,
+    const T y,
     const int width,
     const int height)
 {
@@ -33,34 +42,38 @@ __device__ float bilinear_interp(
   int x2 = ceil(x);
   int y1 = floor(y);
   int y2 = ceil(y);
-  float dist_x = (float)(x - x1);
-  float dist_y = (float)(y - y1);
-  float value11 = data[y1 * width + x1];
-  float value12 = data[y2 * width + x1];
-  float value21 = data[y1 * width + x2];
-  float value22 = data[y2 * width + x2];
-  float value = (1 - dist_x) * (1 - dist_y) * value11 + (1 - dist_x) * dist_y * value12 + dist_x * (1 - dist_y) * value21 + dist_x * dist_y * value22;
+  T dist_x = static_cast<T>(x - x1);
+  T dist_y = static_cast<T>(y - y1);
+  T value11 = data[y1 * width + x1];
+  T value12 = data[y2 * width + x1];
+  T value21 = data[y1 * width + x2];
+  T value22 = data[y2 * width + x2];
+  T value = (1 - dist_x) * (1 - dist_y) * value11 +
+            (1 - dist_x) * dist_y * value12 +
+            dist_x * (1 - dist_y) * value21 +
+            dist_x * dist_y * value22;
   return value;
 }
 
+template <typename T>
 __global__ void DeformablePSROIPoolForwardKernel(
     const int count,
-    const float *bottom_data,
-    const float spatial_scale,
+    const T *bottom_data,
+    const T spatial_scale,
     const int channels,
     const int height, const int width,
     const int pooled_height, const int pooled_width,
-    const float *bottom_rois, const float *bottom_trans,
+    const T *bottom_rois, const T *bottom_trans,
     const int no_trans,
-    const float trans_std,
+    const T trans_std,
     const int sample_per_part,
     const int output_dim,
     const int group_size,
     const int part_size,
     const int num_classes,
     const int channels_each_class,
-    float *top_data,
-    float *top_count)
+    T *top_data,
+    T *top_count)
 {
   CUDA_KERNEL_LOOP(index, count)
   {
@@ -71,49 +84,49 @@ __global__ void DeformablePSROIPoolForwardKernel(
     int n = index / pooled_width / pooled_height / output_dim;
 
     // [start, end) interval for spatial sampling
-    const float *offset_bottom_rois = bottom_rois + n * 5;
+    const T *offset_bottom_rois = bottom_rois + n * 5;
     int roi_batch_ind = offset_bottom_rois[0];
-    float roi_start_w = (float)(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
-    float roi_start_h = (float)(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
-    float roi_end_w = (float)(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
-    float roi_end_h = (float)(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
+    T roi_start_w = static_cast<T>(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
+    T roi_start_h = static_cast<T>(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
+    T roi_end_w = static_cast<T>(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
+    T roi_end_h = static_cast<T>(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
 
     // Force too small ROIs to be 1x1
-    float roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
-    float roi_height = max(roi_end_h - roi_start_h, 0.1);
+    T roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
+    T roi_height = max(roi_end_h - roi_start_h, 0.1);
 
     // Compute w and h at bottom
-    float bin_size_h = roi_height / (float)(pooled_height);
-    float bin_size_w = roi_width / (float)(pooled_width);
+    T bin_size_h = roi_height / static_cast<T>(pooled_height);
+    T bin_size_w = roi_width / static_cast<T>(pooled_width);
 
-    float sub_bin_size_h = bin_size_h / (float)(sample_per_part);
-    float sub_bin_size_w = bin_size_w / (float)(sample_per_part);
+    T sub_bin_size_h = bin_size_h / static_cast<T>(sample_per_part);
+    T sub_bin_size_w = bin_size_w / static_cast<T>(sample_per_part);
 
-    int part_h = floor((float)(ph) / pooled_height * part_size);
-    int part_w = floor((float)(pw) / pooled_width * part_size);
+    int part_h = floor(static_cast<T>(ph) / pooled_height * part_size);
+    int part_w = floor(static_cast<T>(pw) / pooled_width * part_size);
     int class_id = ctop / channels_each_class;
-    float trans_x = no_trans ? (float)(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
-    float trans_y = no_trans ? (float)(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
+    T trans_x = no_trans ? static_cast<T>(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
+    T trans_y = no_trans ? static_cast<T>(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
 
-    float wstart = (float)(pw)*bin_size_w + roi_start_w;
+    T wstart = static_cast<T>(pw) * bin_size_w + roi_start_w;
     wstart += trans_x * roi_width;
-    float hstart = (float)(ph)*bin_size_h + roi_start_h;
+    T hstart = static_cast<T>(ph) * bin_size_h + roi_start_h;
     hstart += trans_y * roi_height;
 
-    float sum = 0;
+    T sum = 0;
     int count = 0;
-    int gw = floor((float)(pw)*group_size / pooled_width);
-    int gh = floor((float)(ph)*group_size / pooled_height);
+    int gw = floor(static_cast<T>(pw) * group_size / pooled_width);
+    int gh = floor(static_cast<T>(ph) * group_size / pooled_height);
     gw = min(max(gw, 0), group_size - 1);
     gh = min(max(gh, 0), group_size - 1);
 
-    const float *offset_bottom_data = bottom_data + (roi_batch_ind * channels) * height * width;
+    const T *offset_bottom_data = bottom_data + (roi_batch_ind * channels) * height * width;
     for (int ih = 0; ih < sample_per_part; ih++)
     {
       for (int iw = 0; iw < sample_per_part; iw++)
       {
-        float w = wstart + iw * sub_bin_size_w;
-        float h = hstart + ih * sub_bin_size_h;
+        T w = wstart + iw * sub_bin_size_w;
+        T h = hstart + ih * sub_bin_size_h;
         // bilinear interpolation
         if (w < -0.5 || w > width - 0.5 || h < -0.5 || h > height - 0.5)
         {
@@ -122,32 +135,33 @@ __global__ void DeformablePSROIPoolForwardKernel(
         w = min(max(w, 0.), width - 1.);
         h = min(max(h, 0.), height - 1.);
         int c = (ctop * group_size + gh) * group_size + gw;
-        float val = bilinear_interp(offset_bottom_data + c * height * width, w, h, width, height);
+        T val = bilinear_interp(offset_bottom_data + c * height * width, w, h, width, height);
         sum += val;
         count++;
       }
     }
-    top_data[index] = count == 0 ? (float)(0) : sum / count;
+    top_data[index] = count == 0 ? static_cast<T>(0) : sum / count;
     top_count[index] = count;
   }
 }
 
+template <typename T>
 __global__ void DeformablePSROIPoolBackwardAccKernel(
     const int count,
-    const float *top_diff,
-    const float *top_count,
+    const T *top_diff,
+    const T *top_count,
     const int num_rois,
-    const float spatial_scale,
+    const T spatial_scale,
     const int channels,
     const int height, const int width,
     const int pooled_height, const int pooled_width,
     const int output_dim,
-    float *bottom_data_diff, float *bottom_trans_diff,
-    const float *bottom_data,
-    const float *bottom_rois,
-    const float *bottom_trans,
+    T *bottom_data_diff, T *bottom_trans_diff,
+    const T *bottom_data,
+    const T *bottom_rois,
+    const T *bottom_trans,
     const int no_trans,
-    const float trans_std,
+    const T trans_std,
     const int sample_per_part,
     const int group_size,
     const int part_size,
@@ -163,44 +177,44 @@ __global__ void DeformablePSROIPoolBackwardAccKernel(
     int n = index / pooled_width / pooled_height / output_dim;
 
     // [start, end) interval for spatial sampling
-    const float *offset_bottom_rois = bottom_rois + n * 5;
+    const T *offset_bottom_rois = bottom_rois + n * 5;
     int roi_batch_ind = offset_bottom_rois[0];
-    float roi_start_w = (float)(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
-    float roi_start_h = (float)(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
-    float roi_end_w = (float)(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
-    float roi_end_h = (float)(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
+    T roi_start_w = static_cast<T>(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
+    T roi_start_h = static_cast<T>(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
+    T roi_end_w = static_cast<T>(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
+    T roi_end_h = static_cast<T>(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
 
     // Force too small ROIs to be 1x1
-    float roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
-    float roi_height = max(roi_end_h - roi_start_h, 0.1);
+    T roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
+    T roi_height = max(roi_end_h - roi_start_h, 0.1);
 
     // Compute w and h at bottom
-    float bin_size_h = roi_height / (float)(pooled_height);
-    float bin_size_w = roi_width / (float)(pooled_width);
+    T bin_size_h = roi_height / static_cast<T>(pooled_height);
+    T bin_size_w = roi_width / static_cast<T>(pooled_width);
 
-    float sub_bin_size_h = bin_size_h / (float)(sample_per_part);
-    float sub_bin_size_w = bin_size_w / (float)(sample_per_part);
+    T sub_bin_size_h = bin_size_h / static_cast<T>(sample_per_part);
+    T sub_bin_size_w = bin_size_w / static_cast<T>(sample_per_part);
 
-    int part_h = floor((float)(ph) / pooled_height * part_size);
-    int part_w = floor((float)(pw) / pooled_width * part_size);
+    int part_h = floor(static_cast<T>(ph) / pooled_height * part_size);
+    int part_w = floor(static_cast<T>(pw) / pooled_width * part_size);
     int class_id = ctop / channels_each_class;
-    float trans_x = no_trans ? (float)(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
-    float trans_y = no_trans ? (float)(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
+    T trans_x = no_trans ? static_cast<T>(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
+    T trans_y = no_trans ? static_cast<T>(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
 
-    float wstart = (float)(pw)*bin_size_w + roi_start_w;
+    T wstart = static_cast<T>(pw) * bin_size_w + roi_start_w;
     wstart += trans_x * roi_width;
-    float hstart = (float)(ph)*bin_size_h + roi_start_h;
+    T hstart = static_cast<T>(ph) * bin_size_h + roi_start_h;
     hstart += trans_y * roi_height;
 
     if (top_count[index] <= 0)
     {
       continue;
     }
-    float diff_val = top_diff[index] / top_count[index];
-    const float *offset_bottom_data = bottom_data + roi_batch_ind * channels * height * width;
-    float *offset_bottom_data_diff = bottom_data_diff + roi_batch_ind * channels * height * width;
-    int gw = floor((float)(pw)*group_size / pooled_width);
-    int gh = floor((float)(ph)*group_size / pooled_height);
+    T diff_val = top_diff[index] / top_count[index];
+    const T *offset_bottom_data = bottom_data + roi_batch_ind * channels * height * width;
+    T *offset_bottom_data_diff = bottom_data_diff + roi_batch_ind * channels * height * width;
+    int gw = floor(static_cast<T>(pw) * group_size / pooled_width);
+    int gh = floor(static_cast<T>(ph) * group_size / pooled_height);
     gw = min(max(gw, 0), group_size - 1);
     gh = min(max(gh, 0), group_size - 1);
 
@@ -208,8 +222,8 @@ __global__ void DeformablePSROIPoolBackwardAccKernel(
     {
       for (int iw = 0; iw < sample_per_part; iw++)
       {
-        float w = wstart + iw * sub_bin_size_w;
-        float h = hstart + ih * sub_bin_size_h;
+        T w = wstart + iw * sub_bin_size_w;
+        T h = hstart + ih * sub_bin_size_h;
         // bilinear interpolation
         if (w < -0.5 || w > width - 0.5 || h < -0.5 || h > height - 0.5)
         {
@@ -223,11 +237,11 @@ __global__ void DeformablePSROIPoolBackwardAccKernel(
         int x1 = ceil(w);
         int y0 = floor(h);
         int y1 = ceil(h);
-        float dist_x = w - x0, dist_y = h - y0;
-        float q00 = (1 - dist_x) * (1 - dist_y);
-        float q01 = (1 - dist_x) * dist_y;
-        float q10 = dist_x * (1 - dist_y);
-        float q11 = dist_x * dist_y;
+        T dist_x = w - x0, dist_y = h - y0;
+        T q00 = (1 - dist_x) * (1 - dist_y);
+        T q01 = (1 - dist_x) * dist_y;
+        T q10 = dist_x * (1 - dist_y);
+        T q11 = dist_x * dist_y;
         int bottom_index_base = c * height * width;
         atomicAdd(offset_bottom_data_diff + bottom_index_base + y0 * width + x0, q00 * diff_val);
         atomicAdd(offset_bottom_data_diff + bottom_index_base + y1 * width + x0, q01 * diff_val);
@@ -238,13 +252,13 @@ __global__ void DeformablePSROIPoolBackwardAccKernel(
         {
           continue;
         }
-        float U00 = offset_bottom_data[bottom_index_base + y0 * width + x0];
-        float U01 = offset_bottom_data[bottom_index_base + y1 * width + x0];
-        float U10 = offset_bottom_data[bottom_index_base + y0 * width + x1];
-        float U11 = offset_bottom_data[bottom_index_base + y1 * width + x1];
-        float diff_x = (U11 * dist_y + U10 * (1 - dist_y) - U01 * dist_y - U00 * (1 - dist_y)) * trans_std * diff_val;
+        T U00 = offset_bottom_data[bottom_index_base + y0 * width + x0];
+        T U01 = offset_bottom_data[bottom_index_base + y1 * width + x0];
+        T U10 = offset_bottom_data[bottom_index_base + y0 * width + x1];
+        T U11 = offset_bottom_data[bottom_index_base + y1 * width + x1];
+        T diff_x = (U11 * dist_y + U10 * (1 - dist_y) - U01 * dist_y - U00 * (1 - dist_y)) * trans_std * diff_val;
         diff_x *= roi_width;
-        float diff_y = (U11 * dist_x + U01 * (1 - dist_x) - U10 * dist_x - U00 * (1 - dist_x)) * trans_std * diff_val;
+        T diff_y = (U11 * dist_x + U01 * (1 - dist_x) - U10 * dist_x - U00 * (1 - dist_x)) * trans_std * diff_val;
         diff_y *= roi_height;
 
         atomicAdd(bottom_trans_diff + (((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w, diff_x);
@@ -254,100 +268,152 @@ __global__ void DeformablePSROIPoolBackwardAccKernel(
   }
 }
 
-void DeformablePSROIPoolForward(cudaStream_t stream,
-                                const float *data,
-                                const float *bbox,
-                                const float *trans,
-                                float *out,
-                                float *top_count,
-                                const int batch,
-                                const int channels,
-                                const int height,
-                                const int width,
-                                const int num_bbox,
-                                const int channels_trans,
-                                const int no_trans,
-                                const float spatial_scale,
-                                const int output_dim,
-                                const int group_size,
-                                const int pooled_size,
-                                const int part_size,
-                                const int sample_per_part,
-                                const float trans_std)
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cuda_forward(const at::Tensor &input,
+                                  const at::Tensor &bbox,
+                                  const at::Tensor &trans,
+                                  const int no_trans,
+                                  const float spatial_scale,
+                                  const int output_dim,
+                                  const int group_size,
+                                  const int pooled_size,
+                                  const int part_size,
+                                  const int sample_per_part,
+                                  const float trans_std)
 {
+  AT_ASSERTM(input.type().is_cuda(), "input must be a CUDA tensor");
+  AT_ASSERTM(bbox.type().is_cuda(), "rois must be a CUDA tensor");
+  AT_ASSERTM(trans.type().is_cuda(), "trans must be a CUDA tensor");
+
+  const int batch = input.size(0);
+  const int channels = input.size(1);
+  const int height = input.size(2);
+  const int width = input.size(3);
+  const int channels_trans = no_trans ? 2 : trans.size(1);
+  const int num_bbox = bbox.size(0);
 
-  const float *bottom_data = data;
-  const float *bottom_rois = bbox;
-  const float *bottom_trans = no_trans ? NULL : trans;
-  float *top_data = out;
-  float *top_count_data = top_count;
+  AT_ASSERTM(channels == output_dim, "input channels and output channels must equal");
+  auto pooled_height = pooled_size;
+  auto pooled_width = pooled_size;
+
+  auto out = at::empty({num_bbox, output_dim, pooled_height, pooled_width}, input.options());
+  long out_size = num_bbox * output_dim * pooled_height * pooled_width;
+  auto top_count = at::zeros({num_bbox, output_dim, pooled_height, pooled_width}, input.options());
 
-  const int pooled_height = pooled_size;
-  const int pooled_width = pooled_size;
-  const int count = num_bbox * output_dim * pooled_height * pooled_width;
   const int num_classes = no_trans ? 1 : channels_trans / 2;
   const int channels_each_class = no_trans ? output_dim : output_dim / num_classes;
 
-  DeformablePSROIPoolForwardKernel<<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, stream>>>(
-      count, bottom_data, spatial_scale, channels, height, width, pooled_height, pooled_width,
-      bottom_rois, bottom_trans, no_trans, trans_std, sample_per_part, output_dim,
-      group_size, part_size, num_classes, channels_each_class, top_data, top_count_data);
+  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
+  if (out.numel() == 0)
   {
-    printf("error in DeformablePSROIPoolForward: %s\n", cudaGetErrorString(err));
+    THCudaCheck(cudaGetLastError());
+    return std::make_tuple(out, top_count);
   }
+
+  dim3 grid(std::min(THCCeilDiv(out_size, 512L), 4096L));
+  dim3 block(512);
+
+  AT_DISPATCH_FLOATING_TYPES(input.type(), "dcn_v2_psroi_pooling_cuda_forward", [&] {
+    DeformablePSROIPoolForwardKernel<scalar_t><<<grid, block, 0, stream>>>(
+        out_size,
+        input.contiguous().data<scalar_t>(),
+        spatial_scale,
+        channels,
+        height, width,
+        pooled_height,
+        pooled_width,
+        bbox.contiguous().data<scalar_t>(),
+        trans.contiguous().data<scalar_t>(),
+        no_trans,
+        trans_std,
+        sample_per_part,
+        output_dim,
+        group_size,
+        part_size,
+        num_classes,
+        channels_each_class,
+        out.data<scalar_t>(),
+        top_count.data<scalar_t>());
+  });
+  THCudaCheck(cudaGetLastError());
+  return std::make_tuple(out, top_count);
 }
 
-void DeformablePSROIPoolBackwardAcc(cudaStream_t stream,
-                                    const float *out_grad,
-                                    const float *data,
-                                    const float *bbox,
-                                    const float *trans,
-                                    const float *top_count,
-                                    float *in_grad,
-                                    float *trans_grad,
-                                    const int batch,
-                                    const int channels,
-                                    const int height,
-                                    const int width,
-                                    const int num_bbox,
-                                    const int channels_trans,
-                                    const int no_trans,
-                                    const float spatial_scale,
-                                    const int output_dim,
-                                    const int group_size,
-                                    const int pooled_size,
-                                    const int part_size,
-                                    const int sample_per_part,
-                                    const float trans_std)
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cuda_backward(const at::Tensor &out_grad,
+                                   const at::Tensor &input,
+                                   const at::Tensor &bbox,
+                                   const at::Tensor &trans,
+                                   const at::Tensor &top_count,
+                                   const int no_trans,
+                                   const float spatial_scale,
+                                   const int output_dim,
+                                   const int group_size,
+                                   const int pooled_size,
+                                   const int part_size,
+                                   const int sample_per_part,
+                                   const float trans_std)
 {
-  // LOG(INFO) << "DeformablePSROIPoolBackward";
-  const float *top_diff = out_grad;
-  const float *bottom_data = data;
-  const float *bottom_rois = bbox;
-  const float *bottom_trans = no_trans ? NULL : trans;
-  float *bottom_data_diff = in_grad;
-  float *bottom_trans_diff = no_trans ? NULL : trans_grad;
-  const float *top_count_data = top_count;
-
-  const int num_rois = num_bbox;
-  const int pooled_height = pooled_size;
-  const int pooled_width = pooled_size;
-  const int count = num_bbox * output_dim * pooled_height * pooled_width;
+  AT_ASSERTM(out_grad.type().is_cuda(), "out_grad must be a CUDA tensor");
+  AT_ASSERTM(input.type().is_cuda(), "input must be a CUDA tensor");
+  AT_ASSERTM(bbox.type().is_cuda(), "bbox must be a CUDA tensor");
+  AT_ASSERTM(trans.type().is_cuda(), "trans must be a CUDA tensor");
+  AT_ASSERTM(top_count.type().is_cuda(), "top_count must be a CUDA tensor");
+
+  const int batch = input.size(0);
+  const int channels = input.size(1);
+  const int height = input.size(2);
+  const int width = input.size(3);
+  const int channels_trans = no_trans ? 2 : trans.size(1);
+  const int num_bbox = bbox.size(0);
+
+  AT_ASSERTM(channels == output_dim, "input channels and output channels must equal");
+  auto pooled_height = pooled_size;
+  auto pooled_width = pooled_size;
+  long out_size = num_bbox * output_dim * pooled_height * pooled_width;
   const int num_classes = no_trans ? 1 : channels_trans / 2;
   const int channels_each_class = no_trans ? output_dim : output_dim / num_classes;
 
-  DeformablePSROIPoolBackwardAccKernel<<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, stream>>>(
-      count, top_diff, top_count_data, num_rois, spatial_scale, channels, height, width,
-      pooled_height, pooled_width, output_dim, bottom_data_diff, bottom_trans_diff,
-      bottom_data, bottom_rois, bottom_trans, no_trans, trans_std, sample_per_part,
-      group_size, part_size, num_classes, channels_each_class);
+  auto input_grad = at::zeros({batch, channels, height, width}, out_grad.options());
+  auto trans_grad = at::zeros_like(trans);
 
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
+  if (input_grad.numel() == 0)
   {
-    printf("error in DeformablePSROIPoolForward: %s\n", cudaGetErrorString(err));
+    THCudaCheck(cudaGetLastError());
+    return std::make_tuple(input_grad, trans_grad);
   }
+
+  dim3 grid(std::min(THCCeilDiv(out_size, 512L), 4096L));
+  dim3 block(512);
+  cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  AT_DISPATCH_FLOATING_TYPES(out_grad.type(), "dcn_v2_psroi_pooling_cuda_backward", [&] {
+    DeformablePSROIPoolBackwardAccKernel<scalar_t><<<grid, block, 0, stream>>>(
+        out_size,
+        out_grad.contiguous().data<scalar_t>(),
+        top_count.contiguous().data<scalar_t>(),
+        num_bbox,
+        spatial_scale,
+        channels,
+        height,
+        width,
+        pooled_height,
+        pooled_width,
+        output_dim,
+        input_grad.contiguous().data<scalar_t>(),
+        trans_grad.contiguous().data<scalar_t>(),
+        input.contiguous().data<scalar_t>(),
+        bbox.contiguous().data<scalar_t>(),
+        trans.contiguous().data<scalar_t>(),
+        no_trans,
+        trans_std,
+        sample_per_part,
+        group_size,
+        part_size,
+        num_classes,
+        channels_each_class);
+  });
+  THCudaCheck(cudaGetLastError());
+  return std::make_tuple(input_grad, trans_grad);
 }
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.h b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.h
deleted file mode 100644
index 5fa2c6c..0000000
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.h
+++ /dev/null
@@ -1,66 +0,0 @@
-/*!
- * Copyright (c) 2017 Microsoft
- * Licensed under The MIT License [see LICENSE for details]
- * \file deformable_psroi_pooling.cu
- * \brief
- * \author Yi Li, Guodong Zhang, Jifeng Dai
-*/
-/***************** Adapted by Charles Shang *********************/
-
-#ifndef DCN_V2_PSROI_POOLING_CUDA
-#define DCN_V2_PSROI_POOLING_CUDA
-
-#ifdef __cplusplus
-extern "C"
-{
-#endif
-
-    void DeformablePSROIPoolForward(cudaStream_t stream,
-                                    const float *data,
-                                    const float *bbox,
-                                    const float *trans,
-                                    float *out,
-                                    float *top_count,
-                                    const int batch,
-                                    const int channels,
-                                    const int height,
-                                    const int width,
-                                    const int num_bbox,
-                                    const int channels_trans,
-                                    const int no_trans,
-                                    const float spatial_scale,
-                                    const int output_dim,
-                                    const int group_size,
-                                    const int pooled_size,
-                                    const int part_size,
-                                    const int sample_per_part,
-                                    const float trans_std);
-
-    void DeformablePSROIPoolBackwardAcc(cudaStream_t stream,
-                                        const float *out_grad,
-                                        const float *data,
-                                        const float *bbox,
-                                        const float *trans,
-                                        const float *top_count,
-                                        float *in_grad,
-                                        float *trans_grad,
-                                        const int batch,
-                                        const int channels,
-                                        const int height,
-                                        const int width,
-                                        const int num_bbox,
-                                        const int channels_trans,
-                                        const int no_trans,
-                                        const float spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const float trans_std);
-
-#ifdef __cplusplus
-}
-#endif
-
-#endif
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.cu b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.cu
deleted file mode 100644
index ce05cc9..0000000
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.cu
+++ /dev/null
@@ -1,368 +0,0 @@
-/*!
- * Copyright (c) 2017 Microsoft
- * Licensed under The MIT License [see LICENSE for details]
- * \file deformable_psroi_pooling.cu
- * \brief
- * \author Yi Li, Guodong Zhang, Jifeng Dai
-*/
-/***************** Adapted by Charles Shang *********************/
-#include "dcn_v2_psroi_pooling_cuda_double.h"
-#include <cstdio>
-#include <algorithm>
-#include <cstring>
-
-#define CUDA_KERNEL_LOOP(i, n)                        \
-  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
-       i < (n);                                       \
-       i += blockDim.x * gridDim.x)
-
-const int CUDA_NUM_THREADS = 1024;
-inline int GET_BLOCKS(const int N)
-{
-  return (N + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS;
-}
-
-#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 600
-#else
-__device__ double atomicAdd(double* address, double val)
-{
-    unsigned long long int* address_as_ull = (unsigned long long int*)address;
-    unsigned long long int old = *address_as_ull, assumed;
-    do {
-        assumed = old;
-        old = atomicCAS(address_as_ull, assumed,
-                __double_as_longlong(val + __longlong_as_double(assumed)));
-    } while (assumed != old);
-    return __longlong_as_double(old);
-}
-#endif
-
-__device__ double bilinear_interp(
-    const double *data,
-    const double x,
-    const double y,
-    const int width,
-    const int height)
-{
-  int x1 = floor(x);
-  int x2 = ceil(x);
-  int y1 = floor(y);
-  int y2 = ceil(y);
-  double dist_x = (double)(x - x1);
-  double dist_y = (double)(y - y1);
-  double value11 = data[y1 * width + x1];
-  double value12 = data[y2 * width + x1];
-  double value21 = data[y1 * width + x2];
-  double value22 = data[y2 * width + x2];
-  double value = (1 - dist_x) * (1 - dist_y) * value11 + (1 - dist_x) * dist_y * value12 + dist_x * (1 - dist_y) * value21 + dist_x * dist_y * value22;
-  return value;
-}
-
-__global__ void DeformablePSROIPoolForwardKernel(
-    const int count,
-    const double *bottom_data,
-    const double spatial_scale,
-    const int channels,
-    const int height, const int width,
-    const int pooled_height, const int pooled_width,
-    const double *bottom_rois, const double *bottom_trans,
-    const int no_trans,
-    const double trans_std,
-    const int sample_per_part,
-    const int output_dim,
-    const int group_size,
-    const int part_size,
-    const int num_classes,
-    const int channels_each_class,
-    double *top_data,
-    double *top_count)
-{
-  CUDA_KERNEL_LOOP(index, count)
-  {
-    // The output is in order (n, ctop, ph, pw)
-    int pw = index % pooled_width;
-    int ph = (index / pooled_width) % pooled_height;
-    int ctop = (index / pooled_width / pooled_height) % output_dim;
-    int n = index / pooled_width / pooled_height / output_dim;
-
-    // [start, end) interval for spatial sampling
-    const double *offset_bottom_rois = bottom_rois + n * 5;
-    int roi_batch_ind = offset_bottom_rois[0];
-    double roi_start_w = (double)(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
-    double roi_start_h = (double)(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
-    double roi_end_w = (double)(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
-    double roi_end_h = (double)(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
-
-    // Force too small ROIs to be 1x1
-    double roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
-    double roi_height = max(roi_end_h - roi_start_h, 0.1);
-
-    // Compute w and h at bottom
-    double bin_size_h = roi_height / (double)(pooled_height);
-    double bin_size_w = roi_width / (double)(pooled_width);
-
-    double sub_bin_size_h = bin_size_h / (double)(sample_per_part);
-    double sub_bin_size_w = bin_size_w / (double)(sample_per_part);
-
-    int part_h = floor((double)(ph) / pooled_height * part_size);
-    int part_w = floor((double)(pw) / pooled_width * part_size);
-    int class_id = ctop / channels_each_class;
-    double trans_x = no_trans ? (double)(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
-    double trans_y = no_trans ? (double)(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
-
-    double wstart = (double)(pw)*bin_size_w + roi_start_w;
-    wstart += trans_x * roi_width;
-    double hstart = (double)(ph)*bin_size_h + roi_start_h;
-    hstart += trans_y * roi_height;
-
-    double sum = 0;
-    int count = 0;
-    int gw = floor((double)(pw)*group_size / pooled_width);
-    int gh = floor((double)(ph)*group_size / pooled_height);
-    gw = min(max(gw, 0), group_size - 1);
-    gh = min(max(gh, 0), group_size - 1);
-
-    const double *offset_bottom_data = bottom_data + (roi_batch_ind * channels) * height * width;
-    for (int ih = 0; ih < sample_per_part; ih++)
-    {
-      for (int iw = 0; iw < sample_per_part; iw++)
-      {
-        double w = wstart + iw * sub_bin_size_w;
-        double h = hstart + ih * sub_bin_size_h;
-        // bilinear interpolation
-        if (w < -0.5 || w > width - 0.5 || h < -0.5 || h > height - 0.5)
-        {
-          continue;
-        }
-        w = min(max(w, 0.), width - 1.);
-        h = min(max(h, 0.), height - 1.);
-        int c = (ctop * group_size + gh) * group_size + gw;
-        double val = bilinear_interp(offset_bottom_data + c * height * width, w, h, width, height);
-        sum += val;
-        count++;
-      }
-    }
-    top_data[index] = count == 0 ? (double)(0) : sum / count;
-    top_count[index] = count;
-  }
-}
-
-__global__ void DeformablePSROIPoolBackwardAccKernel(
-    const int count,
-    const double *top_diff,
-    const double *top_count,
-    const int num_rois,
-    const double spatial_scale,
-    const int channels,
-    const int height, const int width,
-    const int pooled_height, const int pooled_width,
-    const int output_dim,
-    double *bottom_data_diff, double *bottom_trans_diff,
-    const double *bottom_data,
-    const double *bottom_rois,
-    const double *bottom_trans,
-    const int no_trans,
-    const double trans_std,
-    const int sample_per_part,
-    const int group_size,
-    const int part_size,
-    const int num_classes,
-    const int channels_each_class)
-{
-  CUDA_KERNEL_LOOP(index, count)
-  {
-    // The output is in order (n, ctop, ph, pw)
-    int pw = index % pooled_width;
-    int ph = (index / pooled_width) % pooled_height;
-    int ctop = (index / pooled_width / pooled_height) % output_dim;
-    int n = index / pooled_width / pooled_height / output_dim;
-
-    // [start, end) interval for spatial sampling
-    const double *offset_bottom_rois = bottom_rois + n * 5;
-    int roi_batch_ind = offset_bottom_rois[0];
-    double roi_start_w = (double)(round(offset_bottom_rois[1])) * spatial_scale - 0.5;
-    double roi_start_h = (double)(round(offset_bottom_rois[2])) * spatial_scale - 0.5;
-    double roi_end_w = (double)(round(offset_bottom_rois[3]) + 1.) * spatial_scale - 0.5;
-    double roi_end_h = (double)(round(offset_bottom_rois[4]) + 1.) * spatial_scale - 0.5;
-
-    // Force too small ROIs to be 1x1
-    double roi_width = max(roi_end_w - roi_start_w, 0.1); //avoid 0
-    double roi_height = max(roi_end_h - roi_start_h, 0.1);
-
-    // Compute w and h at bottom
-    double bin_size_h = roi_height / (double)(pooled_height);
-    double bin_size_w = roi_width / (double)(pooled_width);
-
-    double sub_bin_size_h = bin_size_h / (double)(sample_per_part);
-    double sub_bin_size_w = bin_size_w / (double)(sample_per_part);
-
-    int part_h = floor((double)(ph) / pooled_height * part_size);
-    int part_w = floor((double)(pw) / pooled_width * part_size);
-    int class_id = ctop / channels_each_class;
-    double trans_x = no_trans ? (double)(0) : bottom_trans[(((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w] * trans_std;
-    double trans_y = no_trans ? (double)(0) : bottom_trans[(((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w] * trans_std;
-
-    double wstart = (double)(pw)*bin_size_w + roi_start_w;
-    wstart += trans_x * roi_width;
-    double hstart = (double)(ph)*bin_size_h + roi_start_h;
-    hstart += trans_y * roi_height;
-
-    if (top_count[index] <= 0)
-    {
-      continue;
-    }
-    double diff_val = top_diff[index] / top_count[index];
-    const double *offset_bottom_data = bottom_data + roi_batch_ind * channels * height * width;
-    double *offset_bottom_data_diff = bottom_data_diff + roi_batch_ind * channels * height * width;
-    int gw = floor((double)(pw)*group_size / pooled_width);
-    int gh = floor((double)(ph)*group_size / pooled_height);
-    gw = min(max(gw, 0), group_size - 1);
-    gh = min(max(gh, 0), group_size - 1);
-
-    for (int ih = 0; ih < sample_per_part; ih++)
-    {
-      for (int iw = 0; iw < sample_per_part; iw++)
-      {
-        double w = wstart + iw * sub_bin_size_w;
-        double h = hstart + ih * sub_bin_size_h;
-        // bilinear interpolation
-        if (w < -0.5 || w > width - 0.5 || h < -0.5 || h > height - 0.5)
-        {
-          continue;
-        }
-        w = min(max(w, 0.), width - 1.);
-        h = min(max(h, 0.), height - 1.);
-        int c = (ctop * group_size + gh) * group_size + gw;
-        // backward on feature
-        int x0 = floor(w);
-        int x1 = ceil(w);
-        int y0 = floor(h);
-        int y1 = ceil(h);
-        double dist_x = w - x0, dist_y = h - y0;
-        double q00 = (1 - dist_x) * (1 - dist_y);
-        double q01 = (1 - dist_x) * dist_y;
-        double q10 = dist_x * (1 - dist_y);
-        double q11 = dist_x * dist_y;
-        int bottom_index_base = c * height * width;
-        atomicAdd(offset_bottom_data_diff + bottom_index_base + y0 * width + x0, q00 * diff_val);
-        atomicAdd(offset_bottom_data_diff + bottom_index_base + y1 * width + x0, q01 * diff_val);
-        atomicAdd(offset_bottom_data_diff + bottom_index_base + y0 * width + x1, q10 * diff_val);
-        atomicAdd(offset_bottom_data_diff + bottom_index_base + y1 * width + x1, q11 * diff_val);
-
-        if (no_trans)
-        {
-          continue;
-        }
-        double U00 = offset_bottom_data[bottom_index_base + y0 * width + x0];
-        double U01 = offset_bottom_data[bottom_index_base + y1 * width + x0];
-        double U10 = offset_bottom_data[bottom_index_base + y0 * width + x1];
-        double U11 = offset_bottom_data[bottom_index_base + y1 * width + x1];
-        double diff_x = (U11 * dist_y + U10 * (1 - dist_y) - U01 * dist_y - U00 * (1 - dist_y)) * trans_std * diff_val;
-        diff_x *= roi_width;
-        double diff_y = (U11 * dist_x + U01 * (1 - dist_x) - U10 * dist_x - U00 * (1 - dist_x)) * trans_std * diff_val;
-        diff_y *= roi_height;
-
-        atomicAdd(bottom_trans_diff + (((n * num_classes + class_id) * 2) * part_size + part_h) * part_size + part_w, diff_x);
-        atomicAdd(bottom_trans_diff + (((n * num_classes + class_id) * 2 + 1) * part_size + part_h) * part_size + part_w, diff_y);
-      }
-    }
-  }
-}
-
-void DeformablePSROIPoolForward(cudaStream_t stream,
-                                const double *data,
-                                const double *bbox,
-                                const double *trans,
-                                double *out,
-                                double *top_count,
-                                const int batch,
-                                const int channels,
-                                const int height,
-                                const int width,
-                                const int num_bbox,
-                                const int channels_trans,
-                                const int no_trans,
-                                const double spatial_scale,
-                                const int output_dim,
-                                const int group_size,
-                                const int pooled_size,
-                                const int part_size,
-                                const int sample_per_part,
-                                const double trans_std)
-{
-
-  const double *bottom_data = data;
-  const double *bottom_rois = bbox;
-  const double *bottom_trans = no_trans ? NULL : trans;
-  double *top_data = out;
-  double *top_count_data = top_count;
-
-  const int pooled_height = pooled_size;
-  const int pooled_width = pooled_size;
-  const int count = num_bbox * output_dim * pooled_height * pooled_width;
-  const int num_classes = no_trans ? 1 : channels_trans / 2;
-  const int channels_each_class = no_trans ? output_dim : output_dim / num_classes;
-
-  DeformablePSROIPoolForwardKernel<<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, stream>>>(
-      count, bottom_data, spatial_scale, channels, height, width, pooled_height, pooled_width,
-      bottom_rois, bottom_trans, no_trans, trans_std, sample_per_part, output_dim,
-      group_size, part_size, num_classes, channels_each_class, top_data, top_count_data);
-
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
-  {
-    printf("error in DeformablePSROIPoolForward: %s\n", cudaGetErrorString(err));
-  }
-}
-
-void DeformablePSROIPoolBackwardAcc(cudaStream_t stream,
-                                    const double *out_grad,
-                                    const double *data,
-                                    const double *bbox,
-                                    const double *trans,
-                                    const double *top_count,
-                                    double *in_grad,
-                                    double *trans_grad,
-                                    const int batch,
-                                    const int channels,
-                                    const int height,
-                                    const int width,
-                                    const int num_bbox,
-                                    const int channels_trans,
-                                    const int no_trans,
-                                    const double spatial_scale,
-                                    const int output_dim,
-                                    const int group_size,
-                                    const int pooled_size,
-                                    const int part_size,
-                                    const int sample_per_part,
-                                    const double trans_std)
-{
-  // LOG(INFO) << "DeformablePSROIPoolBackward";
-  const double *top_diff = out_grad;
-  const double *bottom_data = data;
-  const double *bottom_rois = bbox;
-  const double *bottom_trans = no_trans ? NULL : trans;
-  double *bottom_data_diff = in_grad;
-  double *bottom_trans_diff = no_trans ? NULL : trans_grad;
-  const double *top_count_data = top_count;
-
-  const int num_rois = num_bbox;
-  const int pooled_height = pooled_size;
-  const int pooled_width = pooled_size;
-  const int count = num_bbox * output_dim * pooled_height * pooled_width;
-  const int num_classes = no_trans ? 1 : channels_trans / 2;
-  const int channels_each_class = no_trans ? output_dim : output_dim / num_classes;
-
-  DeformablePSROIPoolBackwardAccKernel<<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, stream>>>(
-      count, top_diff, top_count_data, num_rois, spatial_scale, channels, height, width,
-      pooled_height, pooled_width, output_dim, bottom_data_diff, bottom_trans_diff,
-      bottom_data, bottom_rois, bottom_trans, no_trans, trans_std, sample_per_part,
-      group_size, part_size, num_classes, channels_each_class);
-
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess)
-  {
-    printf("error in DeformablePSROIPoolForward: %s\n", cudaGetErrorString(err));
-  }
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.h b/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.h
deleted file mode 100644
index 8a16f72..0000000
--- a/src/lib/models/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda_double.h
+++ /dev/null
@@ -1,66 +0,0 @@
-/*!
- * Copyright (c) 2017 Microsoft
- * Licensed under The MIT License [see LICENSE for details]
- * \file deformable_psroi_pooling.cu
- * \brief
- * \author Yi Li, Guodong Zhang, Jifeng Dai
-*/
-/***************** Adapted by Charles Shang *********************/
-
-#ifndef DCN_V2_PSROI_POOLING_CUDA_DOUBLE
-#define DCN_V2_PSROI_POOLING_CUDA_DOUBLE
-
-#ifdef __cplusplus
-extern "C"
-{
-#endif
-
-    void DeformablePSROIPoolForward(cudaStream_t stream,
-                                    const double *data,
-                                    const double *bbox,
-                                    const double *trans,
-                                    double *out,
-                                    double *top_count,
-                                    const int batch,
-                                    const int channels,
-                                    const int height,
-                                    const int width,
-                                    const int num_bbox,
-                                    const int channels_trans,
-                                    const int no_trans,
-                                    const double spatial_scale,
-                                    const int output_dim,
-                                    const int group_size,
-                                    const int pooled_size,
-                                    const int part_size,
-                                    const int sample_per_part,
-                                    const double trans_std);
-
-    void DeformablePSROIPoolBackwardAcc(cudaStream_t stream,
-                                        const double *out_grad,
-                                        const double *data,
-                                        const double *bbox,
-                                        const double *trans,
-                                        const double *top_count,
-                                        double *in_grad,
-                                        double *trans_grad,
-                                        const int batch,
-                                        const int channels,
-                                        const int height,
-                                        const int width,
-                                        const int num_bbox,
-                                        const int channels_trans,
-                                        const int no_trans,
-                                        const double spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const double trans_std);
-
-#ifdef __cplusplus
-}
-#endif
-
-#endif
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/cuda/vision.h b/src/lib/models/networks/DCNv2/src/cuda/vision.h
new file mode 100644
index 0000000..e42a2a7
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/src/cuda/vision.h
@@ -0,0 +1,60 @@
+#pragma once
+#include <torch/extension.h>
+
+at::Tensor
+dcn_v2_cuda_forward(const at::Tensor &input,
+                    const at::Tensor &weight,
+                    const at::Tensor &bias,
+                    const at::Tensor &offset,
+                    const at::Tensor &mask,
+                    const int kernel_h,
+                    const int kernel_w,
+                    const int stride_h,
+                    const int stride_w,
+                    const int pad_h,
+                    const int pad_w,
+                    const int dilation_h,
+                    const int dilation_w,
+                    const int deformable_group);
+
+std::vector<at::Tensor>
+dcn_v2_cuda_backward(const at::Tensor &input,
+                     const at::Tensor &weight,
+                     const at::Tensor &bias,
+                     const at::Tensor &offset,
+                     const at::Tensor &mask,
+                     const at::Tensor &grad_output,
+                     int kernel_h, int kernel_w,
+                     int stride_h, int stride_w,
+                     int pad_h, int pad_w,
+                     int dilation_h, int dilation_w,
+                     int deformable_group);
+
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cuda_forward(const at::Tensor &input,
+                                  const at::Tensor &bbox,
+                                  const at::Tensor &trans,
+                                  const int no_trans,
+                                  const float spatial_scale,
+                                  const int output_dim,
+                                  const int group_size,
+                                  const int pooled_size,
+                                  const int part_size,
+                                  const int sample_per_part,
+                                  const float trans_std);
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_cuda_backward(const at::Tensor &out_grad,
+                                   const at::Tensor &input,
+                                   const at::Tensor &bbox,
+                                   const at::Tensor &trans,
+                                   const at::Tensor &top_count,
+                                   const int no_trans,
+                                   const float spatial_scale,
+                                   const int output_dim,
+                                   const int group_size,
+                                   const int pooled_size,
+                                   const int part_size,
+                                   const int sample_per_part,
+                                   const float trans_std);
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2.c b/src/lib/models/networks/DCNv2/src/dcn_v2.c
deleted file mode 100644
index b440d3f..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2.c
+++ /dev/null
@@ -1,30 +0,0 @@
-#include <TH/TH.h>
-#include <stdio.h>
-#include <math.h>
-
-void dcn_v2_forward(THFloatTensor *input, THFloatTensor *weight,
-                        THFloatTensor *bias, THFloatTensor *ones,
-                        THFloatTensor *offset, THFloatTensor *mask,
-                        THFloatTensor *output, THFloatTensor *columns,
-                        const int pad_h, const int pad_w,
-                        const int stride_h, const int stride_w,
-                        const int dilation_h, const int dilation_w,
-                        const int deformable_group)
-{
-    printf("only implemented in GPU");
-}
-    void dcn_v2_backward(THFloatTensor *input, THFloatTensor *weight,
-                         THFloatTensor *bias, THFloatTensor *ones,
-                         THFloatTensor *offset, THFloatTensor *mask,
-                         THFloatTensor *output, THFloatTensor *columns,
-                         THFloatTensor *grad_input, THFloatTensor *grad_weight,
-                         THFloatTensor *grad_bias, THFloatTensor *grad_offset,
-                         THFloatTensor *grad_mask, THFloatTensor *grad_output,
-                         int kernel_h, int kernel_w,
-                         int stride_h, int stride_w,
-                         int pad_h, int pad_w,
-                         int dilation_h, int dilation_w,
-                         int deformable_group)
-{
-    printf("only implemented in GPU");
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2.h b/src/lib/models/networks/DCNv2/src/dcn_v2.h
index 1a97ff0..23f5caf 100644
--- a/src/lib/models/networks/DCNv2/src/dcn_v2.h
+++ b/src/lib/models/networks/DCNv2/src/dcn_v2.h
@@ -1,20 +1,145 @@
-void dcn_v2_forward(THFloatTensor *input, THFloatTensor *weight,
-                        THFloatTensor *bias, THFloatTensor *ones,
-                        THFloatTensor *offset, THFloatTensor *mask,
-                        THFloatTensor *output, THFloatTensor *columns,
-                        const int pad_h, const int pad_w,
-                        const int stride_h, const int stride_w,
-                        const int dilation_h, const int dilation_w,
-                        const int deformable_group);
-void dcn_v2_backward(THFloatTensor *input, THFloatTensor *weight,
-                        THFloatTensor *bias, THFloatTensor *ones,
-                        THFloatTensor *offset, THFloatTensor *mask,
-                        THFloatTensor *output, THFloatTensor *columns,
-                        THFloatTensor *grad_input, THFloatTensor *grad_weight,
-                        THFloatTensor *grad_bias, THFloatTensor *grad_offset,
-                        THFloatTensor *grad_mask, THFloatTensor *grad_output,
-                        int kernel_h, int kernel_w,
-                        int stride_h, int stride_w,
-                        int pad_h, int pad_w,
-                        int dilation_h, int dilation_w,
-                        int deformable_group);
\ No newline at end of file
+#pragma once
+
+#include "cpu/vision.h"
+
+#ifdef WITH_CUDA
+#include "cuda/vision.h"
+#endif
+
+at::Tensor
+dcn_v2_forward(const at::Tensor &input,
+               const at::Tensor &weight,
+               const at::Tensor &bias,
+               const at::Tensor &offset,
+               const at::Tensor &mask,
+               const int kernel_h,
+               const int kernel_w,
+               const int stride_h,
+               const int stride_w,
+               const int pad_h,
+               const int pad_w,
+               const int dilation_h,
+               const int dilation_w,
+               const int deformable_group)
+{
+    if (input.type().is_cuda())
+    {
+#ifdef WITH_CUDA
+        return dcn_v2_cuda_forward(input, weight, bias, offset, mask,
+                                   kernel_h, kernel_w,
+                                   stride_h, stride_w,
+                                   pad_h, pad_w,
+                                   dilation_h, dilation_w,
+                                   deformable_group);
+#else
+        AT_ERROR("Not compiled with GPU support");
+#endif
+    }
+    AT_ERROR("Not implemented on the CPU");
+}
+
+std::vector<at::Tensor>
+dcn_v2_backward(const at::Tensor &input,
+                const at::Tensor &weight,
+                const at::Tensor &bias,
+                const at::Tensor &offset,
+                const at::Tensor &mask,
+                const at::Tensor &grad_output,
+                int kernel_h, int kernel_w,
+                int stride_h, int stride_w,
+                int pad_h, int pad_w,
+                int dilation_h, int dilation_w,
+                int deformable_group)
+{
+    if (input.type().is_cuda())
+    {
+#ifdef WITH_CUDA
+        return dcn_v2_cuda_backward(input,
+                                    weight,
+                                    bias,
+                                    offset,
+                                    mask,
+                                    grad_output,
+                                    kernel_h, kernel_w,
+                                    stride_h, stride_w,
+                                    pad_h, pad_w,
+                                    dilation_h, dilation_w,
+                                    deformable_group);
+#else
+        AT_ERROR("Not compiled with GPU support");
+#endif
+    }
+    AT_ERROR("Not implemented on the CPU");
+}
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_forward(const at::Tensor &input,
+                             const at::Tensor &bbox,
+                             const at::Tensor &trans,
+                             const int no_trans,
+                             const float spatial_scale,
+                             const int output_dim,
+                             const int group_size,
+                             const int pooled_size,
+                             const int part_size,
+                             const int sample_per_part,
+                             const float trans_std)
+{
+    if (input.type().is_cuda())
+    {
+#ifdef WITH_CUDA
+        return dcn_v2_psroi_pooling_cuda_forward(input,
+                                                 bbox,
+                                                 trans,
+                                                 no_trans,
+                                                 spatial_scale,
+                                                 output_dim,
+                                                 group_size,
+                                                 pooled_size,
+                                                 part_size,
+                                                 sample_per_part,
+                                                 trans_std);
+#else
+        AT_ERROR("Not compiled with GPU support");
+#endif
+    }
+    AT_ERROR("Not implemented on the CPU");
+}
+
+std::tuple<at::Tensor, at::Tensor>
+dcn_v2_psroi_pooling_backward(const at::Tensor &out_grad,
+                              const at::Tensor &input,
+                              const at::Tensor &bbox,
+                              const at::Tensor &trans,
+                              const at::Tensor &top_count,
+                              const int no_trans,
+                              const float spatial_scale,
+                              const int output_dim,
+                              const int group_size,
+                              const int pooled_size,
+                              const int part_size,
+                              const int sample_per_part,
+                              const float trans_std)
+{
+    if (input.type().is_cuda())
+    {
+#ifdef WITH_CUDA
+        return dcn_v2_psroi_pooling_cuda_backward(out_grad,
+                                                  input,
+                                                  bbox,
+                                                  trans,
+                                                  top_count,
+                                                  no_trans,
+                                                  spatial_scale,
+                                                  output_dim,
+                                                  group_size,
+                                                  pooled_size,
+                                                  part_size,
+                                                  sample_per_part,
+                                                  trans_std);
+#else
+        AT_ERROR("Not compiled with GPU support");
+#endif
+    }
+    AT_ERROR("Not implemented on the CPU");
+}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.c b/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.c
deleted file mode 100644
index 1503b5d..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.c
+++ /dev/null
@@ -1,335 +0,0 @@
-#include <THC/THC.h>
-#include "cuda/dcn_v2_im2col_cuda.h"
-#include "cuda/dcn_v2_psroi_pooling_cuda.h"
-
-extern THCState *state;
-
-// author: Charles Shang
-// https://github.com/torch/cunn/blob/master/lib/THCUNN/generic/SpatialConvolutionMM.cu
-
-void dcn_v2_cuda_forward(THCudaTensor *input, THCudaTensor *weight,
-                         THCudaTensor *bias, THCudaTensor *ones,
-                         THCudaTensor *offset, THCudaTensor *mask,
-                         THCudaTensor *output, THCudaTensor *columns,
-                         int kernel_h, int kernel_w,
-                         const int stride_h, const int stride_w,
-                         const int pad_h, const int pad_w,
-                         const int dilation_h, const int dilation_w,
-                         const int deformable_group)
-{
-    THCAssertSameGPU(THCudaTensor_checkGPU(state, 8, input, weight, bias, ones, offset, mask, output, columns));
-    THArgCheck(THCudaTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THArgCheck(THCudaTensor_isContiguous(state, weight), 2, "weight tensor has to be contiguous");
-    
-    const int batch = THCudaTensor_size(state, input, 0);
-    const int channels = THCudaTensor_size(state, input, 1);
-    const int height = THCudaTensor_size(state, input, 2);
-    const int width = THCudaTensor_size(state, input, 3);
-
-    const int channels_out = THCudaTensor_size(state, weight, 0);
-    const int channels_kernel = THCudaTensor_size(state, weight, 1);
-    const int kernel_h_ = THCudaTensor_size(state, weight, 2);
-    const int kernel_w_ = THCudaTensor_size(state, weight, 3);
-    if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
-        THError("Input shape and kernel shape wont match: (%d x %d vs %d x %d).", 
-        kernel_h_, kernel_w, kernel_h_, kernel_w_);
-    if (channels != channels_kernel)
-        THError("Input shape and kernel channels wont match: (%d vs %d).", 
-        channels, channels_kernel);
-
-    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
-    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
-
-    if (THCudaTensor_nDimension(state, ones) != 2 ||
-        THCudaTensor_size(state, ones, 0) * THCudaTensor_size(state, ones, 1) < height_out * width_out)
-    {
-        // Resize plane and fill with ones...
-        THCudaTensor_resize2d(state, ones, height_out, width_out);
-        THCudaTensor_fill(state, ones, 1);
-    }
-
-    // resize output
-    THCudaTensor_resize4d(state, output, batch, channels_out, height_out, width_out);
-    // resize temporary columns
-    THCudaTensor_resize2d(state, columns, channels * kernel_h * kernel_w, 1 * height_out * width_out);
-
-    THCudaTensor *input_n = THCudaTensor_new(state);
-    THCudaTensor *offset_n = THCudaTensor_new(state);
-    THCudaTensor *mask_n = THCudaTensor_new(state);
-    THCudaTensor *output_n = THCudaTensor_new(state);
-
-    for (int b = 0; b < batch; b++)
-    {
-        THCudaTensor_select(state, input_n, input, 0, b);
-        THCudaTensor_select(state, offset_n, offset, 0, b);
-        THCudaTensor_select(state, mask_n, mask, 0, b);
-        THCudaTensor_select(state, output_n, output, 0, b);
-
-        // Do Bias first:
-        // M,N,K are dims of matrix A and B
-        // (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)
-        // (N x 1) (1 x M)
-        long m_ = channels_out;
-        long n_ = height_out * width_out;
-        long k_ = 1;
-        THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,
-                         THCudaTensor_data(state, ones), k_,
-                         THCudaTensor_data(state, bias), k_, 0.0f,
-                         THCudaTensor_data(state, output_n), n_);
-
-        modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
-                                         THCudaTensor_data(state, input_n), THCudaTensor_data(state, offset_n),
-                                         THCudaTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,
-                                         deformable_group, THCudaTensor_data(state, columns));
-
-        //(k * m)  x  (m * n)
-        // Y = WC
-        long m = channels_out;
-        long n = height_out * width_out;
-        long k = channels * kernel_h * kernel_w;
-        THCudaBlas_Sgemm(state, 'n', 'n', n, m, k, 1.0f,
-                         THCudaTensor_data(state, columns), n,
-                         THCudaTensor_data(state, weight), k, 1.0f,
-                         THCudaTensor_data(state, output_n), n);
-    }
-    THCudaTensor_free(state, input_n);
-    THCudaTensor_free(state, offset_n);
-    THCudaTensor_free(state, mask_n);
-    THCudaTensor_free(state, output_n);
-}
-
-void dcn_v2_cuda_backward(THCudaTensor *input, THCudaTensor *weight,
-                          THCudaTensor *bias, THCudaTensor *ones,
-                          THCudaTensor *offset, THCudaTensor *mask,
-                          THCudaTensor *columns,
-                          THCudaTensor *grad_input, THCudaTensor *grad_weight,
-                          THCudaTensor *grad_bias, THCudaTensor *grad_offset,
-                          THCudaTensor *grad_mask, THCudaTensor *grad_output,
-                          int kernel_h, int kernel_w,
-                          int stride_h, int stride_w,
-                          int pad_h, int pad_w,
-                          int dilation_h, int dilation_w,
-                          int deformable_group)
-{
-    THCAssertSameGPU(THCudaTensor_checkGPU(state, 13, input, weight, bias, ones, offset, mask, columns,
-                                           grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output));
-    THArgCheck(THCudaTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THArgCheck(THCudaTensor_isContiguous(state, weight), 2, "weight tensor has to be contiguous");
-
-    const int batch = THCudaTensor_size(state, input, 0);
-    const int channels = THCudaTensor_size(state, input, 1);
-    const int height = THCudaTensor_size(state, input, 2);
-    const int width = THCudaTensor_size(state, input, 3);
-
-    const int channels_out = THCudaTensor_size(state, weight, 0);
-    const int channels_kernel = THCudaTensor_size(state, weight, 1);
-    const int kernel_h_ = THCudaTensor_size(state, weight, 2);
-    const int kernel_w_ = THCudaTensor_size(state, weight, 3);
-    if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
-        THError("Input shape and kernel shape wont match: (%d x %d vs %d x %d).", 
-        kernel_h_, kernel_w, kernel_h_, kernel_w_);
-    if (channels != channels_kernel)
-        THError("Input shape and kernel channels wont match: (%d vs %d).", 
-        channels, channels_kernel);
-
-    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
-    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
-
-    if (THCudaTensor_nDimension(state, ones) != 2 ||
-        THCudaTensor_size(state, ones, 0) * THCudaTensor_size(state, ones, 1) < height_out * width_out)
-    {
-        // Resize plane and fill with ones...
-        THCudaTensor_resize2d(state, ones, height_out, width_out);
-        THCudaTensor_fill(state, ones, 1.0f);
-    }
-
-    THCudaTensor_resize4d(state, grad_input, batch, channels, height, width);
-    THCudaTensor_resize2d(state, columns, channels * kernel_h * kernel_w, height_out * width_out);
-
-    THCudaTensor *input_n = THCudaTensor_new(state);
-    THCudaTensor *offset_n = THCudaTensor_new(state);
-    THCudaTensor *mask_n = THCudaTensor_new(state);
-
-    THCudaTensor *grad_output_n = THCudaTensor_new(state);
-    THCudaTensor *grad_input_n = THCudaTensor_new(state);
-    THCudaTensor *grad_offset_n = THCudaTensor_new(state);
-    THCudaTensor *grad_mask_n = THCudaTensor_new(state);
-
-    for (int b = 0; b < batch; b++)
-    {
-        THCudaTensor_select(state, input_n, input, 0, b);
-        THCudaTensor_select(state, offset_n, offset, 0, b);
-        THCudaTensor_select(state, mask_n, mask, 0, b);
-        THCudaTensor_select(state, grad_output_n, grad_output, 0, b);
-        THCudaTensor_select(state, grad_input_n, grad_input, 0, b);
-        THCudaTensor_select(state, grad_offset_n, grad_offset, 0, b);
-        THCudaTensor_select(state, grad_mask_n, grad_mask, 0, b);
-
-        long m = channels * kernel_h * kernel_w;
-        long n = height_out * width_out;
-        long k = channels_out;
-
-        THCudaBlas_Sgemm(state, 'n', 't', n, m, k, 1.0f,
-                         THCudaTensor_data(state, grad_output_n), n,
-                         THCudaTensor_data(state, weight), m, 0.0f,
-                         THCudaTensor_data(state, columns), n);
-
-        // gradient w.r.t. input coordinate data
-        modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),
-                                               THCudaTensor_data(state, columns),
-                                               THCudaTensor_data(state, input_n),
-                                               THCudaTensor_data(state, offset_n),
-                                               THCudaTensor_data(state, mask_n),
-                                               1, channels, height, width,
-                                               height_out, width_out, kernel_h, kernel_w,
-                                               pad_h, pad_w, stride_h, stride_w,
-                                               dilation_h, dilation_w, deformable_group,
-                                               THCudaTensor_data(state, grad_offset_n),
-                                               THCudaTensor_data(state, grad_mask_n));
-        // gradient w.r.t. input data
-        modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),
-                                         THCudaTensor_data(state, columns),
-                                         THCudaTensor_data(state, offset_n),
-                                         THCudaTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w,
-                                         dilation_h, dilation_w, deformable_group,
-                                         THCudaTensor_data(state, grad_input_n));
-
-        // gradient w.r.t. weight, dWeight should accumulate across the batch and group
-        modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
-                                         THCudaTensor_data(state, input_n),
-                                         THCudaTensor_data(state, offset_n),
-                                         THCudaTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w,
-                                         dilation_h, dilation_w, deformable_group,
-                                         THCudaTensor_data(state, columns));
-        long m_ = channels_out;
-        long n_ = channels * kernel_h * kernel_w;
-        long k_ = height_out * width_out;
-
-        THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,
-                         THCudaTensor_data(state, columns), k_,
-                         THCudaTensor_data(state, grad_output_n), k_, 1.0f,
-                         THCudaTensor_data(state, grad_weight), n_);
-
-        // gradient w.r.t. bias
-        // long m_ = channels_out;
-        // long k__ = height_out * width_out;
-        THCudaBlas_Sgemv(state,
-                         't',
-                         k_, m_, 1.0f,
-                         THCudaTensor_data(state, grad_output_n), k_,
-                         THCudaTensor_data(state, ones), 1, 1.0f,
-                         THCudaTensor_data(state, grad_bias), 1);
-    }
-
-    THCudaTensor_free(state, input_n);
-    THCudaTensor_free(state, offset_n);
-    THCudaTensor_free(state, mask_n);
-
-    THCudaTensor_free(state, grad_output_n);
-    THCudaTensor_free(state, grad_input_n);
-    THCudaTensor_free(state, grad_offset_n);
-    THCudaTensor_free(state, grad_mask_n);
-}
-
-void dcn_v2_psroi_pooling_cuda_forward(THCudaTensor * input, THCudaTensor * bbox,
-                                       THCudaTensor * trans, 
-                                       THCudaTensor * out, THCudaTensor * top_count,
-                                       const int no_trans,
-                                       const float spatial_scale,
-                                       const int output_dim,
-                                       const int group_size,
-                                       const int pooled_size,
-                                       const int part_size,
-                                       const int sample_per_part,
-                                       const float trans_std)
-{
-    THArgCheck(THCudaTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THCAssertSameGPU(THCudaTensor_checkGPU(state, 5, input, bbox, trans, out, top_count));
-
-    const int batch = THCudaTensor_size(state, input, 0);
-    const int channels = THCudaTensor_size(state, input, 1);
-    const int height = THCudaTensor_size(state, input, 2);
-    const int width = THCudaTensor_size(state, input, 3);
-    const int channels_trans = no_trans? 2 : THCudaTensor_size(state, trans, 1);
-
-    const int num_bbox = THCudaTensor_size(state, bbox, 0);
-    if (num_bbox != THCudaTensor_size(state, out, 0))
-        THError("Output shape and bbox number wont match: (%d vs %d).", 
-                THCudaTensor_size(state, out, 0), num_bbox);
-
-    DeformablePSROIPoolForward(THCState_getCurrentStream(state),
-                               THCudaTensor_data(state, input),
-                               THCudaTensor_data(state, bbox),
-                               THCudaTensor_data(state, trans),
-                               THCudaTensor_data(state, out),
-                               THCudaTensor_data(state, top_count),
-                               batch, channels, height, width,
-                               num_bbox, 
-                               channels_trans, 
-                               no_trans, 
-                               spatial_scale,
-                               output_dim, 
-                               group_size, 
-                               pooled_size, 
-                               part_size,
-                               sample_per_part, 
-                               trans_std);
-}
-
-void dcn_v2_psroi_pooling_cuda_backward(THCudaTensor * out_grad, 
-                                        THCudaTensor * input, THCudaTensor * bbox,
-                                        THCudaTensor * trans, THCudaTensor * top_count,
-                                        THCudaTensor * input_grad, THCudaTensor * trans_grad,
-                                        const int no_trans,
-                                        const float spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const float trans_std)
-{
-    THArgCheck(THCudaTensor_isContiguous(state, out_grad), 0, "out_grad tensor has to be contiguous");
-    THArgCheck(THCudaTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THCAssertSameGPU(THCudaTensor_checkGPU(state, 7, input, bbox, trans, out_grad, top_count,
-                    input_grad, trans_grad));
-
-    const int batch = THCudaTensor_size(state, input, 0);
-    const int channels = THCudaTensor_size(state, input, 1);
-    const int height = THCudaTensor_size(state, input, 2);
-    const int width = THCudaTensor_size(state, input, 3);
-    const int channels_trans = no_trans? 2 : THCudaTensor_size(state, trans, 1);
-
-    const int num_bbox = THCudaTensor_size(state, bbox, 0);
-    if (num_bbox != THCudaTensor_size(state, out_grad, 0))
-        THError("Output shape and bbox number wont match: (%d vs %d).", 
-                THCudaTensor_size(state, out_grad, 0), num_bbox);
-
-    DeformablePSROIPoolBackwardAcc(THCState_getCurrentStream(state),
-                                   THCudaTensor_data(state, out_grad),
-                                   THCudaTensor_data(state, input),
-                                   THCudaTensor_data(state, bbox),
-                                   THCudaTensor_data(state, trans),
-                                   THCudaTensor_data(state, top_count),
-                                   THCudaTensor_data(state, input_grad),
-                                   THCudaTensor_data(state, trans_grad),
-                                   batch, channels, height, width, num_bbox,
-                                   channels_trans, 
-                                   no_trans, 
-                                   spatial_scale, 
-                                   output_dim,
-                                   group_size, 
-                                   pooled_size, 
-                                   part_size,
-                                   sample_per_part, 
-                                   trans_std);
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.h b/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.h
deleted file mode 100644
index 70a27a8..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda.h
+++ /dev/null
@@ -1,60 +0,0 @@
-// #ifndef DCN_V2_CUDA
-// #define DCN_V2_CUDA
-
-// #ifdef __cplusplus
-// extern "C"
-// {
-// #endif
-
-void dcn_v2_cuda_forward(THCudaTensor *input, THCudaTensor *weight,
-                         THCudaTensor *bias, THCudaTensor *ones,
-                         THCudaTensor *offset, THCudaTensor *mask,
-                         THCudaTensor *output, THCudaTensor *columns,
-                         int kernel_h, int kernel_w,
-                         const int stride_h, const int stride_w,
-                         const int pad_h, const int pad_w,
-                         const int dilation_h, const int dilation_w,
-                         const int deformable_group);
-void dcn_v2_cuda_backward(THCudaTensor *input, THCudaTensor *weight,
-                          THCudaTensor *bias, THCudaTensor *ones,
-                          THCudaTensor *offset, THCudaTensor *mask,
-                          THCudaTensor *columns,
-                          THCudaTensor *grad_input, THCudaTensor *grad_weight,
-                          THCudaTensor *grad_bias, THCudaTensor *grad_offset,
-                          THCudaTensor *grad_mask, THCudaTensor *grad_output,
-                          int kernel_h, int kernel_w,
-                          int stride_h, int stride_w,
-                          int pad_h, int pad_w,
-                          int dilation_h, int dilation_w,
-                          int deformable_group);
-
-void dcn_v2_psroi_pooling_cuda_forward(THCudaTensor * input, THCudaTensor * bbox,
-                                       THCudaTensor * trans, 
-                                       THCudaTensor * out, THCudaTensor * top_count,
-                                       const int no_trans,
-                                       const float spatial_scale,
-                                       const int output_dim,
-                                       const int group_size,
-                                       const int pooled_size,
-                                       const int part_size,
-                                       const int sample_per_part,
-                                       const float trans_std);
-
-void dcn_v2_psroi_pooling_cuda_backward(THCudaTensor * out_grad, 
-                                        THCudaTensor * input, THCudaTensor * bbox,
-                                        THCudaTensor * trans, THCudaTensor * top_count,
-                                        THCudaTensor * input_grad, THCudaTensor * trans_grad,
-                                        const int no_trans,
-                                        const float spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const float trans_std);
-
-// #ifdef __cplusplus
-// }
-// #endif
-
-// #endif
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.c b/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.c
deleted file mode 100644
index 021ef12..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.c
+++ /dev/null
@@ -1,358 +0,0 @@
-#include <THC/THC.h>
-#include "cuda/dcn_v2_im2col_cuda_double.h"
-#include "cuda/dcn_v2_psroi_pooling_cuda_double.h"
-
-extern THCState *state;
-
-// author: Charles Shang
-// https://github.com/torch/cunn/blob/master/lib/THCUNN/generic/SpatialConvolutionMM.cu
-
-void dcn_v2_cuda_forward(THCudaDoubleTensor *input, THCudaDoubleTensor *weight,
-                         THCudaDoubleTensor *bias, THCudaDoubleTensor *ones,
-                         THCudaDoubleTensor *offset, THCudaDoubleTensor *mask,
-                         THCudaDoubleTensor *output, THCudaDoubleTensor *columns,
-                         int kernel_h, int kernel_w,
-                         const int stride_h, const int stride_w,
-                         const int pad_h, const int pad_w,
-                         const int dilation_h, const int dilation_w,
-                         const int deformable_group)
-{
-    THCAssertSameGPU(THCudaDoubleTensor_checkGPU(state, 8, input, weight, bias, ones, offset, mask, output, columns));
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, weight), 2, "weight tensor has to be contiguous");
-
-    input = THCudaDoubleTensor_newContiguous(state, input);
-    offset = THCudaDoubleTensor_newContiguous(state, offset);
-    mask = THCudaDoubleTensor_newContiguous(state, mask);
-    weight = THCudaDoubleTensor_newContiguous(state, weight);
-
-    const int batch = THCudaDoubleTensor_size(state, input, 0);
-    const int channels = THCudaDoubleTensor_size(state, input, 1);
-    const int height = THCudaDoubleTensor_size(state, input, 2);
-    const int width = THCudaDoubleTensor_size(state, input, 3);
-
-    const int channels_out = THCudaDoubleTensor_size(state, weight, 0);
-    const int channels_kernel = THCudaDoubleTensor_size(state, weight, 1);
-    const int kernel_h_ = THCudaDoubleTensor_size(state, weight, 2);
-    const int kernel_w_ = THCudaDoubleTensor_size(state, weight, 3);
-    if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
-        THError("Input shape and kernel shape wont match: (%d x %d vs %d x %d).",
-                kernel_h_, kernel_w, kernel_h_, kernel_w_);
-    if (channels != channels_kernel)
-        THError("Input shape and kernel channels wont match: (%d vs %d).",
-                channels, channels_kernel);
-
-    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
-    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
-
-    if (THCudaDoubleTensor_nDimension(state, ones) != 2 ||
-        THCudaDoubleTensor_size(state, ones, 0) * THCudaDoubleTensor_size(state, ones, 1) < height_out * width_out)
-    {
-        // Resize plane and fill with ones...
-        THCudaDoubleTensor_resize2d(state, ones, height_out, width_out);
-        THCudaDoubleTensor_fill(state, ones, 1);
-    }
-
-    // resize output
-    THCudaDoubleTensor_resize4d(state, output, batch, channels_out, height_out, width_out);
-    // resize temporary columns
-    THCudaDoubleTensor_resize2d(state, columns, channels * kernel_h * kernel_w, 1 * height_out * width_out);
-
-    THCudaDoubleTensor *input_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *offset_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *mask_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *output_n = THCudaDoubleTensor_new(state);
-
-    for (int b = 0; b < batch; b++)
-    {
-        THCudaDoubleTensor_select(state, input_n, input, 0, b);
-        THCudaDoubleTensor_select(state, offset_n, offset, 0, b);
-        THCudaDoubleTensor_select(state, mask_n, mask, 0, b);
-        THCudaDoubleTensor_select(state, output_n, output, 0, b);
-
-        // Do Bias first:
-        // M,N,K are dims of matrix A and B
-        // (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)
-        // (N x 1) (1 x M)
-        long m_ = channels_out;
-        long n_ = height_out * width_out;
-        long k_ = 1;
-        THCudaBlas_Dgemm(state, 't', 'n', n_, m_, k_, 1.0,
-                         THCudaDoubleTensor_data(state, ones), k_,
-                         THCudaDoubleTensor_data(state, bias), k_, 0.0,
-                         THCudaDoubleTensor_data(state, output_n), n_);
-
-        modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
-                                         THCudaDoubleTensor_data(state, input_n), THCudaDoubleTensor_data(state, offset_n),
-                                         THCudaDoubleTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,
-                                         deformable_group, THCudaDoubleTensor_data(state, columns));
-
-        //(k * m)  x  (m * n)
-        // Y = WC
-        long m = channels_out;
-        long n = height_out * width_out;
-        long k = channels * kernel_h * kernel_w;
-        THCudaBlas_Dgemm(state, 'n', 'n', n, m, k, 1.0f,
-                         THCudaDoubleTensor_data(state, columns), n,
-                         THCudaDoubleTensor_data(state, weight), k, 1.0f,
-                         THCudaDoubleTensor_data(state, output_n), n);
-    }
-    THCudaDoubleTensor_free(state, input_n);
-    THCudaDoubleTensor_free(state, offset_n);
-    THCudaDoubleTensor_free(state, mask_n);
-    THCudaDoubleTensor_free(state, output_n);
-
-    THCudaDoubleTensor_free(state, input);
-    THCudaDoubleTensor_free(state, offset);
-    THCudaDoubleTensor_free(state, mask);
-    THCudaDoubleTensor_free(state, weight);
-}
-
-void dcn_v2_cuda_backward(THCudaDoubleTensor *input, THCudaDoubleTensor *weight,
-                          THCudaDoubleTensor *bias, THCudaDoubleTensor *ones,
-                          THCudaDoubleTensor *offset, THCudaDoubleTensor *mask,
-                          THCudaDoubleTensor *columns,
-                          THCudaDoubleTensor *grad_input, THCudaDoubleTensor *grad_weight,
-                          THCudaDoubleTensor *grad_bias, THCudaDoubleTensor *grad_offset,
-                          THCudaDoubleTensor *grad_mask, THCudaDoubleTensor *grad_output,
-                          int kernel_h, int kernel_w,
-                          int stride_h, int stride_w,
-                          int pad_h, int pad_w,
-                          int dilation_h, int dilation_w,
-                          int deformable_group)
-{
-    THCAssertSameGPU(THCudaDoubleTensor_checkGPU(state, 13, input, weight, bias, ones, offset, mask, columns,
-                                                 grad_input, grad_weight, grad_bias, grad_offset, grad_mask, grad_output));
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, weight), 2, "weight tensor has to be contiguous");
-
-    input = THCudaDoubleTensor_newContiguous(state, input);
-    offset = THCudaDoubleTensor_newContiguous(state, offset);
-    mask = THCudaDoubleTensor_newContiguous(state, mask);
-    weight = THCudaDoubleTensor_newContiguous(state, weight);
-    grad_output = THCudaDoubleTensor_newContiguous(state, grad_output);
-
-    const int batch = THCudaDoubleTensor_size(state, input, 0);
-    const int channels = THCudaDoubleTensor_size(state, input, 1);
-    const int height = THCudaDoubleTensor_size(state, input, 2);
-    const int width = THCudaDoubleTensor_size(state, input, 3);
-
-    const int channels_out = THCudaDoubleTensor_size(state, weight, 0);
-    const int channels_kernel = THCudaDoubleTensor_size(state, weight, 1);
-    const int kernel_h_ = THCudaDoubleTensor_size(state, weight, 2);
-    const int kernel_w_ = THCudaDoubleTensor_size(state, weight, 3);
-    if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
-        THError("Input shape and kernel shape wont match: (%d x %d vs %d x %d).",
-                kernel_h_, kernel_w, kernel_h_, kernel_w_);
-    if (channels != channels_kernel)
-        THError("Input shape and kernel channels wont match: (%d vs %d).",
-                channels, channels_kernel);
-
-    const int height_out = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
-    const int width_out = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;
-
-    if (THCudaDoubleTensor_nDimension(state, ones) != 2 ||
-        THCudaDoubleTensor_size(state, ones, 0) * THCudaDoubleTensor_size(state, ones, 1) < height_out * width_out)
-    {
-        // Resize plane and fill with ones...
-        THCudaDoubleTensor_resize2d(state, ones, height_out, width_out);
-        THCudaDoubleTensor_fill(state, ones, 1);
-    }
-
-    // THCudaDoubleTensor_resize4d(state, grad_input, batch, channels, height, width);
-    THCudaDoubleTensor_resize2d(state, columns, channels * kernel_h * kernel_w, height_out * width_out);
-
-    THCudaDoubleTensor *input_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *offset_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *mask_n = THCudaDoubleTensor_new(state);
-
-    THCudaDoubleTensor *grad_output_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *grad_input_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *grad_offset_n = THCudaDoubleTensor_new(state);
-    THCudaDoubleTensor *grad_mask_n = THCudaDoubleTensor_new(state);
-
-    for (int b = 0; b < batch; b++)
-    {
-        THCudaDoubleTensor_select(state, input_n, input, 0, b);
-        THCudaDoubleTensor_select(state, offset_n, offset, 0, b);
-        THCudaDoubleTensor_select(state, mask_n, mask, 0, b);
-        THCudaDoubleTensor_select(state, grad_output_n, grad_output, 0, b);
-        THCudaDoubleTensor_select(state, grad_input_n, grad_input, 0, b);
-        THCudaDoubleTensor_select(state, grad_offset_n, grad_offset, 0, b);
-        THCudaDoubleTensor_select(state, grad_mask_n, grad_mask, 0, b);
-
-        long m = channels * kernel_h * kernel_w;
-        long n = height_out * width_out;
-        long k = channels_out;
-
-        THCudaBlas_Dgemm(state, 'n', 't', n, m, k, 1.0,
-                         THCudaDoubleTensor_data(state, grad_output_n), n,
-                         THCudaDoubleTensor_data(state, weight), m, 0.0,
-                         THCudaDoubleTensor_data(state, columns), n);
-
-        // gradient w.r.t. input offset and mask data
-        modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),
-                                               THCudaDoubleTensor_data(state, columns),
-                                               THCudaDoubleTensor_data(state, input_n),
-                                               THCudaDoubleTensor_data(state, offset_n),
-                                               THCudaDoubleTensor_data(state, mask_n),
-                                               1, channels, height, width,
-                                               height_out, width_out, kernel_h, kernel_w,
-                                               pad_h, pad_w, stride_h, stride_w,
-                                               dilation_h, dilation_w, deformable_group,
-                                               THCudaDoubleTensor_data(state, grad_offset_n),
-                                               THCudaDoubleTensor_data(state, grad_mask_n));
-        // gradient w.r.t. input data
-        modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),
-                                         THCudaDoubleTensor_data(state, columns),
-                                         THCudaDoubleTensor_data(state, offset_n),
-                                         THCudaDoubleTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w,
-                                         dilation_h, dilation_w, deformable_group,
-                                         THCudaDoubleTensor_data(state, grad_input_n));
-
-        // gradient w.r.t. weight, dWeight should accumulate across the batch and group
-        modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),
-                                         THCudaDoubleTensor_data(state, input_n),
-                                         THCudaDoubleTensor_data(state, offset_n),
-                                         THCudaDoubleTensor_data(state, mask_n),
-                                         1, channels, height, width,
-                                         height_out, width_out, kernel_h, kernel_w,
-                                         pad_h, pad_w, stride_h, stride_w,
-                                         dilation_h, dilation_w, deformable_group,
-                                         THCudaDoubleTensor_data(state, columns));
-        long m_ = channels_out;
-        long n_ = channels * kernel_h * kernel_w;
-        long k_ = height_out * width_out;
-
-        THCudaBlas_Dgemm(state, 't', 'n', n_, m_, k_, 1.0,
-                         THCudaDoubleTensor_data(state, columns), k_,
-                         THCudaDoubleTensor_data(state, grad_output_n), k_, 1.0,
-                         THCudaDoubleTensor_data(state, grad_weight), n_);
-
-        // gradient w.r.t. bias
-        // long m_ = channels_out;
-        // long k__ = height_out * width_out;
-        THCudaBlas_Dgemv(state,
-                         't',
-                         k_, m_, 1.0,
-                         THCudaDoubleTensor_data(state, grad_output_n), k_,
-                         THCudaDoubleTensor_data(state, ones), 1, 1.0,
-                         THCudaDoubleTensor_data(state, grad_bias), 1);
-    }
-
-    THCudaDoubleTensor_free(state, input_n);
-    THCudaDoubleTensor_free(state, offset_n);
-    THCudaDoubleTensor_free(state, mask_n);
-
-    THCudaDoubleTensor_free(state, grad_output_n);
-    THCudaDoubleTensor_free(state, grad_input_n);
-    THCudaDoubleTensor_free(state, grad_offset_n);
-    THCudaDoubleTensor_free(state, grad_mask_n);
-
-    THCudaDoubleTensor_free(state, input);
-    THCudaDoubleTensor_free(state, offset);
-    THCudaDoubleTensor_free(state, mask);
-    THCudaDoubleTensor_free(state, weight);
-    THCudaDoubleTensor_free(state, grad_output);
-}
-
-
-void dcn_v2_psroi_pooling_cuda_forward(THCudaDoubleTensor * input, THCudaDoubleTensor * bbox,
-                                       THCudaDoubleTensor * trans, 
-                                       THCudaDoubleTensor * out, THCudaDoubleTensor * top_count,
-                                       const int no_trans,
-                                       const double spatial_scale,
-                                       const int output_dim,
-                                       const int group_size,
-                                       const int pooled_size,
-                                       const int part_size,
-                                       const int sample_per_part,
-                                       const double trans_std)
-{
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THCAssertSameGPU(THCudaDoubleTensor_checkGPU(state, 5, input, bbox, trans, out, top_count));
-
-    const int batch = THCudaDoubleTensor_size(state, input, 0);
-    const int channels = THCudaDoubleTensor_size(state, input, 1);
-    const int height = THCudaDoubleTensor_size(state, input, 2);
-    const int width = THCudaDoubleTensor_size(state, input, 3);
-    const int channels_trans = no_trans? 2 : THCudaDoubleTensor_size(state, trans, 1);
-
-    const int num_bbox = THCudaDoubleTensor_size(state, bbox, 0);
-    if (num_bbox != THCudaDoubleTensor_size(state, out, 0))
-        THError("Output shape and bbox number wont match: (%d vs %d).", 
-                THCudaDoubleTensor_size(state, out, 0), num_bbox);
-
-    DeformablePSROIPoolForward(THCState_getCurrentStream(state),
-                               THCudaDoubleTensor_data(state, input),
-                               THCudaDoubleTensor_data(state, bbox),
-                               THCudaDoubleTensor_data(state, trans),
-                               THCudaDoubleTensor_data(state, out),
-                               THCudaDoubleTensor_data(state, top_count),
-                               batch, channels, height, width,
-                               num_bbox, 
-                               channels_trans, 
-                               no_trans, 
-                               spatial_scale,
-                               output_dim, 
-                               group_size, 
-                               pooled_size, 
-                               part_size,
-                               sample_per_part, 
-                               trans_std);
-}
-
-void dcn_v2_psroi_pooling_cuda_backward(THCudaDoubleTensor * out_grad, 
-                                        THCudaDoubleTensor * input, THCudaDoubleTensor * bbox,
-                                        THCudaDoubleTensor * trans, THCudaDoubleTensor * top_count,
-                                        THCudaDoubleTensor * input_grad, THCudaDoubleTensor * trans_grad,
-                                        const int no_trans,
-                                        const double spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const double trans_std)
-{
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, out_grad), 0, "out_grad tensor has to be contiguous");
-    THArgCheck(THCudaDoubleTensor_isContiguous(state, input), 1, "input tensor has to be contiguous");
-    THCAssertSameGPU(THCudaDoubleTensor_checkGPU(state, 7, input, bbox, trans, out_grad, top_count,
-                    input_grad, trans_grad));
-
-    const int batch = THCudaDoubleTensor_size(state, input, 0);
-    const int channels = THCudaDoubleTensor_size(state, input, 1);
-    const int height = THCudaDoubleTensor_size(state, input, 2);
-    const int width = THCudaDoubleTensor_size(state, input, 3);
-    const int channels_trans = no_trans? 2 : THCudaDoubleTensor_size(state, trans, 1);
-
-    const int num_bbox = THCudaDoubleTensor_size(state, bbox, 0);
-    if (num_bbox != THCudaDoubleTensor_size(state, out_grad, 0))
-        THError("Output shape and bbox number wont match: (%d vs %d).", 
-                THCudaDoubleTensor_size(state, out_grad, 0), num_bbox);
-
-    DeformablePSROIPoolBackwardAcc(THCState_getCurrentStream(state),
-                                   THCudaDoubleTensor_data(state, out_grad),
-                                   THCudaDoubleTensor_data(state, input),
-                                   THCudaDoubleTensor_data(state, bbox),
-                                   THCudaDoubleTensor_data(state, trans),
-                                   THCudaDoubleTensor_data(state, top_count),
-                                   THCudaDoubleTensor_data(state, input_grad),
-                                   THCudaDoubleTensor_data(state, trans_grad),
-                                   batch, channels, height, width, num_bbox,
-                                   channels_trans, 
-                                   no_trans, 
-                                   spatial_scale, 
-                                   output_dim,
-                                   group_size, 
-                                   pooled_size, 
-                                   part_size,
-                                   sample_per_part, 
-                                   trans_std);
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.h b/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.h
deleted file mode 100644
index 826cb2b..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_cuda_double.h
+++ /dev/null
@@ -1,61 +0,0 @@
-// #ifndef DCN_V2_CUDA
-// #define DCN_V2_CUDA
-
-// #ifdef __cplusplus
-// extern "C"
-// {
-// #endif
-
-void dcn_v2_cuda_forward(THCudaDoubleTensor *input, THCudaDoubleTensor *weight,
-                         THCudaDoubleTensor *bias, THCudaDoubleTensor *ones,
-                         THCudaDoubleTensor *offset, THCudaDoubleTensor *mask,
-                         THCudaDoubleTensor *output, THCudaDoubleTensor *columns,
-                         int kernel_h, int kernel_w,
-                         const int stride_h, const int stride_w,
-                         const int pad_h, const int pad_w,
-                         const int dilation_h, const int dilation_w,
-                         const int deformable_group);
-void dcn_v2_cuda_backward(THCudaDoubleTensor *input, THCudaDoubleTensor *weight,
-                          THCudaDoubleTensor *bias, THCudaDoubleTensor *ones,
-                          THCudaDoubleTensor *offset, THCudaDoubleTensor *mask,
-                          THCudaDoubleTensor *columns,
-                          THCudaDoubleTensor *grad_input, THCudaDoubleTensor *grad_weight,
-                          THCudaDoubleTensor *grad_bias, THCudaDoubleTensor *grad_offset,
-                          THCudaDoubleTensor *grad_mask, THCudaDoubleTensor *grad_output,
-                          int kernel_h, int kernel_w,
-                          int stride_h, int stride_w,
-                          int pad_h, int pad_w,
-                          int dilation_h, int dilation_w,
-                          int deformable_group);
-
-void dcn_v2_psroi_pooling_cuda_forward(THCudaDoubleTensor * input, THCudaDoubleTensor * bbox,
-                                       THCudaDoubleTensor * trans, 
-                                       THCudaDoubleTensor * out, THCudaDoubleTensor * top_count,
-                                       const int no_trans,
-                                       const double spatial_scale,
-                                       const int output_dim,
-                                       const int group_size,
-                                       const int pooled_size,
-                                       const int part_size,
-                                       const int sample_per_part,
-                                       const double trans_std);
-
-void dcn_v2_psroi_pooling_cuda_backward(THCudaDoubleTensor * out_grad, 
-                                        THCudaDoubleTensor * input, THCudaDoubleTensor * bbox,
-                                        THCudaDoubleTensor * trans, THCudaDoubleTensor * top_count,
-                                        THCudaDoubleTensor * input_grad, THCudaDoubleTensor * trans_grad,
-                                        const int no_trans,
-                                        const double spatial_scale,
-                                        const int output_dim,
-                                        const int group_size,
-                                        const int pooled_size,
-                                        const int part_size,
-                                        const int sample_per_part,
-                                        const double trans_std);
-
-
-// #ifdef __cplusplus
-// }
-// #endif
-
-// #endif
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_double.c b/src/lib/models/networks/DCNv2/src/dcn_v2_double.c
deleted file mode 100644
index 2b86545..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_double.c
+++ /dev/null
@@ -1,30 +0,0 @@
-#include <TH/TH.h>
-#include <stdio.h>
-#include <math.h>
-
-void dcn_v2_forward(THDoubleTensor *input, THDoubleTensor *weight,
-                    THDoubleTensor *bias, THDoubleTensor *ones,
-                    THDoubleTensor *offset, THDoubleTensor *mask,
-                    THDoubleTensor *output, THDoubleTensor *columns,
-                    const int pad_h, const int pad_w,
-                    const int stride_h, const int stride_w,
-                    const int dilation_h, const int dilation_w,
-                    const int deformable_group)
-{
-    printf("only implemented in GPU");
-}
-void dcn_v2_backward(THDoubleTensor *input, THDoubleTensor *weight,
-                     THDoubleTensor *bias, THDoubleTensor *ones,
-                     THDoubleTensor *offset, THDoubleTensor *mask,
-                     THDoubleTensor *output, THDoubleTensor *columns,
-                     THDoubleTensor *grad_input, THDoubleTensor *grad_weight,
-                     THDoubleTensor *grad_bias, THDoubleTensor *grad_offset,
-                     THDoubleTensor *grad_mask, THDoubleTensor *grad_output,
-                     int kernel_h, int kernel_w,
-                     int stride_h, int stride_w,
-                     int pad_h, int pad_w,
-                     int dilation_h, int dilation_w,
-                     int deformable_group)
-{
-    printf("only implemented in GPU");
-}
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/dcn_v2_double.h b/src/lib/models/networks/DCNv2/src/dcn_v2_double.h
deleted file mode 100644
index eda1f4c..0000000
--- a/src/lib/models/networks/DCNv2/src/dcn_v2_double.h
+++ /dev/null
@@ -1,20 +0,0 @@
-void dcn_v2_forward(THDoubleTensor *input, THDoubleTensor *weight,
-                    THDoubleTensor *bias, THDoubleTensor *ones,
-                    THDoubleTensor *offset, THDoubleTensor *mask,
-                    THDoubleTensor *output, THDoubleTensor *columns,
-                    const int pad_h, const int pad_w,
-                    const int stride_h, const int stride_w,
-                    const int dilation_h, const int dilation_w,
-                    const int deformable_group);
-void dcn_v2_backward(THDoubleTensor *input, THDoubleTensor *weight,
-                     THDoubleTensor *bias, THDoubleTensor *ones,
-                     THDoubleTensor *offset, THDoubleTensor *mask,
-                     THDoubleTensor *output, THDoubleTensor *columns,
-                     THDoubleTensor *grad_input, THDoubleTensor *grad_weight,
-                     THDoubleTensor *grad_bias, THDoubleTensor *grad_offset,
-                     THDoubleTensor *grad_mask, THDoubleTensor *grad_output,
-                     int kernel_h, int kernel_w,
-                     int stride_h, int stride_w,
-                     int pad_h, int pad_w,
-                     int dilation_h, int dilation_w,
-                     int deformable_group);
\ No newline at end of file
diff --git a/src/lib/models/networks/DCNv2/src/vision.cpp b/src/lib/models/networks/DCNv2/src/vision.cpp
new file mode 100644
index 0000000..ff54233
--- /dev/null
+++ b/src/lib/models/networks/DCNv2/src/vision.cpp
@@ -0,0 +1,9 @@
+
+#include "dcn_v2.h"
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  m.def("dcn_v2_forward", &dcn_v2_forward, "dcn_v2_forward");
+  m.def("dcn_v2_backward", &dcn_v2_backward, "dcn_v2_backward");
+  m.def("dcn_v2_psroi_pooling_forward", &dcn_v2_psroi_pooling_forward, "dcn_v2_psroi_pooling_forward");
+  m.def("dcn_v2_psroi_pooling_backward", &dcn_v2_psroi_pooling_backward, "dcn_v2_psroi_pooling_backward");
+}
diff --git a/src/lib/models/networks/DCNv2/test.py b/src/lib/models/networks/DCNv2/test.py
index 3a8b2e4..3bd5bd2 100644
--- a/src/lib/models/networks/DCNv2/test.py
+++ b/src/lib/models/networks/DCNv2/test.py
@@ -8,16 +8,15 @@ import torch
 import torch.nn as nn
 from torch.autograd import gradcheck
 
-from dcn_v2 import DCNv2
-from dcn_v2_func import DCNv2Function
-from dcn_v2 import DCNv2Pooling
-from dcn_v2_func import DCNv2PoolingFunction
+from dcn_v2 import dcn_v2_conv, DCNv2, DCN
+from dcn_v2 import dcn_v2_pooling, DCNv2Pooling, DCNPooling
 
 deformable_groups = 1
 N, inC, inH, inW = 2, 2, 4, 4
 outC = 2
 kH, kW = 3, 3
 
+
 def conv_identify(weight, bias):
     weight.data.zero_()
     bias.data.zero_()
@@ -29,18 +28,19 @@ def conv_identify(weight, bias):
             if p == q:
                 weight.data[q, p, y, x] = 1.0
 
+
 def check_zero_offset():
     conv_offset = nn.Conv2d(inC, deformable_groups * 2 * kH * kW,
-        kernel_size=(kH, kW),
-        stride=(1, 1),
-        padding=(1, 1),
-        bias=True).cuda()
+                            kernel_size=(kH, kW),
+                            stride=(1, 1),
+                            padding=(1, 1),
+                            bias=True).cuda()
 
     conv_mask = nn.Conv2d(inC, deformable_groups * 1 * kH * kW,
-        kernel_size=(kH, kW),
-        stride=(1, 1),
-        padding=(1, 1),
-        bias=True).cuda()
+                          kernel_size=(kH, kW),
+                          stride=(1, 1),
+                          padding=(1, 1),
+                          bias=True).cuda()
 
     dcn_v2 = DCNv2(inC, outC, (kH, kW),
                    stride=1, padding=1, dilation=1,
@@ -63,38 +63,15 @@ def check_zero_offset():
         print('Zero offset passed')
     else:
         print('Zero offset failed')
-
-def check_gradient_dconv_double():
-
-    input = torch.randn(N, inC, inH, inW, dtype=torch.float64).cuda()
-    input.requires_grad = True
-
-    offset = torch.randn(N, deformable_groups * 2 * kW * kH, inH, inW, dtype=torch.float64).cuda()
-    # offset.data.zero_()
-    # offset.data -= 0.00001
-    offset.requires_grad = True
-
-    mask = torch.rand(N, deformable_groups * 1 * kW * kH, inH, inW, dtype=torch.float64).cuda()
-    # mask.data.zero_()
-    mask.requires_grad = True
-    mask = torch.sigmoid(mask)
-
-    weight = torch.randn(outC, inC, kH, kW, dtype=torch.float64).cuda()
-    weight.requires_grad = True
-
-    bias = torch.rand(outC, dtype=torch.float64).cuda()
-    bias.requires_grad = True
-
-    func = DCNv2Function(stride=1, padding=1, dilation=1, deformable_groups=deformable_groups)
-
-    print(gradcheck(func, (input, offset, mask, weight, bias), eps=1e-6, atol=1e-5, rtol=1e-3))
+        print(input)
+        print(output)
 
 def check_gradient_dconv():
 
-    input = torch.randn(N, inC, inH, inW).cuda()
+    input = torch.rand(N, inC, inH, inW).cuda() * 0.01
     input.requires_grad = True
 
-    offset = torch.randn(N, deformable_groups * 2 * kW * kH, inH, inW).cuda()
+    offset = torch.randn(N, deformable_groups * 2 * kW * kH, inH, inW).cuda() * 2
     # offset.data.zero_()
     # offset.data -= 0.5
     offset.requires_grad = True
@@ -110,12 +87,18 @@ def check_gradient_dconv():
     bias = torch.rand(outC).cuda()
     bias.requires_grad = True
 
-    func = DCNv2Function(stride=1, padding=1, dilation=1, deformable_groups=deformable_groups)
+    stride = 1
+    padding = 1
+    dilation = 1
+
+    print('check_gradient_dconv: ',
+          gradcheck(dcn_v2_conv, (input, offset, mask, weight, bias,
+                    stride, padding, dilation, deformable_groups),
+                    eps=1e-3, atol=1e-4, rtol=1e-2))
 
-    print(gradcheck(func, (input, offset, mask, weight, bias), eps=1e-3, atol=1e-3, rtol=1e-2))
 
 def check_pooling_zero_offset():
-    from dcn_v2 import DCNv2Pooling
+
     input = torch.randn(2, 16, 64, 64).cuda().zero_()
     input[0, :, 16:26, 16:26] = 1.
     input[1, :, 10:20, 20:30] = 2.
@@ -128,10 +111,11 @@ def check_pooling_zero_offset():
                            output_dim=16,
                            no_trans=True,
                            group_size=1,
-                           trans_std=0.1).cuda()
+                           trans_std=0.0).cuda()
 
     out = pooling(input, rois, input.new())
-    s = ', '.join(['%f' % out[i, :, :, :].mean().item() for i in range(rois.shape[0])])
+    s = ', '.join(['%f' % out[i, :, :, :].mean().item()
+                   for i in range(rois.shape[0])])
     print(s)
 
     dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,
@@ -139,12 +123,14 @@ def check_pooling_zero_offset():
                             output_dim=16,
                             no_trans=False,
                             group_size=1,
-                            trans_std=0.1).cuda()
+                            trans_std=0.0).cuda()
     offset = torch.randn(20, 2, 7, 7).cuda().zero_()
     dout = dpooling(input, rois, offset)
-    s = ', '.join(['%f' % dout[i, :, :, :].mean().item() for i in range(rois.shape[0])])
+    s = ', '.join(['%f' % dout[i, :, :, :].mean().item()
+                   for i in range(rois.shape[0])])
     print(s)
 
+
 def check_gradient_dpooling():
     input = torch.randn(2, 3, 5, 5).cuda() * 0.01
     N = 4
@@ -155,22 +141,37 @@ def check_gradient_dpooling():
     h = torch.rand((N, 1)).cuda().float() * 10
     rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)
     offset = torch.randn(N, 2, 3, 3).cuda()
-    dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,
-                            pooled_size=3,
-                            output_dim=3,
-                            no_trans=False,
-                            group_size=1,
-                            trans_std=0.0).cuda()
     input.requires_grad = True
     offset.requires_grad = True
-    print('check_gradient_dpooling', gradcheck(dpooling, (input, rois, offset), eps=1e-4))
+
+    spatial_scale = 1.0 / 4
+    pooled_size = 3
+    output_dim = 3
+    no_trans = 0
+    group_size = 1
+    trans_std = 0.0
+    sample_per_part = 4
+    part_size = pooled_size
+
+    print('check_gradient_dpooling:',
+          gradcheck(dcn_v2_pooling, (input, rois, offset,
+                                     spatial_scale,
+                                     pooled_size,
+                                     output_dim,
+                                     no_trans,
+                                     group_size,
+                                     part_size,
+                                     sample_per_part,
+                                     trans_std),
+                    eps=1e-4))
 
 
 def example_dconv():
-    from dcn_v2 import DCN
     input = torch.randn(2, 64, 128, 128).cuda()
     # wrap all things (offset and mask) in DCN
-    dcn = DCN(64, 64, kernel_size=(3,3), stride=1, padding=1, deformable_groups=2).cuda()
+    dcn = DCN(64, 64, kernel_size=(3, 3), stride=1,
+              padding=1, deformable_groups=2).cuda()
+    # print(dcn.weight.shape, input.shape)
     output = dcn(input)
     targert = output.new(*output.size())
     targert.data.uniform_(-0.01, 0.01)
@@ -178,8 +179,8 @@ def example_dconv():
     error.backward()
     print(output.shape)
 
+
 def example_dpooling():
-    from dcn_v2 import DCNv2Pooling
     input = torch.randn(2, 32, 64, 64).cuda()
     batch_inds = torch.randint(2, (20, 1)).cuda().float()
     x = torch.randint(256, (20, 1)).cuda().float()
@@ -221,8 +222,8 @@ def example_dpooling():
     e = (target_dout - dout).mean()
     e.backward()
 
+
 def example_mdpooling():
-    from dcn_v2 import DCNPooling
     input = torch.randn(2, 32, 64, 64).cuda()
     input.requires_grad = True
     batch_inds = torch.randint(2, (20, 1)).cuda().float()
@@ -234,11 +235,12 @@ def example_mdpooling():
 
     # mdformable pooling (V2)
     dpooling = DCNPooling(spatial_scale=1.0 / 4,
-                         pooled_size=7,
-                         output_dim=32,
-                         no_trans=False,
-                         group_size=1,
-                         trans_std=0.1).cuda()
+                          pooled_size=7,
+                          output_dim=32,
+                          no_trans=False,
+                          group_size=1,
+                          trans_std=0.1,
+                          deform_fc_dim=1024).cuda()
 
     dout = dpooling(input, rois)
     target = dout.new(*dout.size())
@@ -247,6 +249,7 @@ def example_mdpooling():
     error.backward()
     print(dout.shape)
 
+
 if __name__ == '__main__':
 
     example_dconv()
@@ -259,19 +262,9 @@ if __name__ == '__main__':
         check_zero_offset()
 
     check_gradient_dpooling()
-
-    # # gradient check
-    # try:
-    #     check_gradient_double()
-    # except TypeError:
-    #     print('''****** You can swith to double precision in dcn_v2_func.py by (un)commenting these two lines:
-    #              ****** from _ext import dcn_v2 as _backend
-    #              ****** from _ext import dcn_v2_double as _backend''')
-    #     print('****** Your tensor may not be **double** type')
-    #     print('****** Switching to **float** type')
-    #
-    #     check_gradient()
-    # finally:
-    #     print('****** Note: backward is not reentrant error may not be a serious problem, '
-    #           '****** since the max error is less than 1e-7\n'
-    #           '****** Still looking for what trigger this problem')
\ No newline at end of file
+    check_gradient_dconv()
+    # """
+    # ****** Note: backward is not reentrant error may not be a serious problem,
+    # ****** since the max error is less than 1e-7,
+    # ****** Still looking for what trigger this problem
+    # """
-- 
2.17.1

